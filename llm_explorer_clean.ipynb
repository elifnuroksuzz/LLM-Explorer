{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 128090,
          "status": "ok",
          "timestamp": 1752823714320,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "Bk-AK4MEbN4l",
        "outputId": "b5e4a44a-f38c-476d-c1d4-9ee6d44866ad"
      },
      "outputs": [],
      "source": [
        "# T\u00fcm gerekli k\u00fct\u00fcphaneleri tek komutla kur\n",
        "!pip install transformers torch gradio matplotlib seaborn pandas numpy tqdm plotly ipywidgets tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT_GzgX4bWrl"
      },
      "source": [
        "Hugging Face: do\u011fal dil i\u015fleme (NLP) i\u00e7in a\u00e7\u0131k kaynakl\u0131 modeller, veri setleri ve ara\u00e7lar sunan bir platformdur. \u00d6zellikle transformers k\u00fct\u00fcphanesiyle, BERT, GPT gibi \u00f6nceden e\u011fitilmi\u015f modelleri kolayca kullanmay\u0131 sa\u011flar.\n",
        "\n",
        "transformers: Hugging Face'in do\u011fal dil i\u015fleme modellerini (\u00f6rn. BERT, GPT) kullanmak i\u00e7in k\u00fct\u00fcphane.\n",
        "\n",
        "torch: PyTorch, derin \u00f6\u011frenme modelleri olu\u015fturmak ve e\u011fitmek i\u00e7in kullan\u0131lan bir makine \u00f6\u011frenimi \u00e7er\u00e7evesi.\n",
        "\n",
        "gradio: Makine \u00f6\u011frenimi modelleri i\u00e7in web tabanl\u0131 kullan\u0131c\u0131 aray\u00fczleri olu\u015fturmay\u0131 sa\u011flayan k\u00fct\u00fcphane.\n",
        "\n",
        "matplotlib: Veri g\u00f6rselle\u015ftirme i\u00e7in 2D grafikler ve \u00e7izimler olu\u015fturan k\u00fct\u00fcphane.\n",
        "\n",
        "seaborn: Matplotlib tabanl\u0131, istatistiksel veri g\u00f6rselle\u015ftirme i\u00e7in daha estetik grafikler sunan k\u00fct\u00fcphane.\n",
        "\n",
        "pandas: Veri analizi ve manip\u00fclasyonu i\u00e7in g\u00fc\u00e7l\u00fc veri yap\u0131lar\u0131 (DataFrame) sa\u011flayan k\u00fct\u00fcphane.\n",
        "\n",
        "numpy: Say\u0131sal hesaplamalar i\u00e7in h\u0131zl\u0131 dizi i\u015flemleri ve matematiksel fonksiyonlar sunar.\n",
        "\n",
        "tqdm: D\u00f6ng\u00fclerde ilerleme \u00e7ubu\u011fu g\u00f6stererek i\u015flem ilerlemesini takip etmeyi sa\u011flar.\n",
        "\n",
        "plotly: Etkile\u015fimli ve dinamik veri g\u00f6rselle\u015ftirme grafikleri olu\u015fturmak i\u00e7in k\u00fct\u00fcphane.\n",
        "\n",
        "ipywidgets: Jupyter Notebook'larda etkile\u015fimli widget'lar (d\u00fc\u011fmeler, kayd\u0131r\u0131c\u0131lar) olu\u015fturur.\n",
        "\n",
        "tokenizers: Metin tokenizasyonu i\u00e7in h\u0131zl\u0131 ve \u00f6zelle\u015ftirilebilir ara\u00e7lar sa\u011flayan k\u00fct\u00fcphane"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19pjL3xDbYfK"
      },
      "outputs": [],
      "source": [
        "#K\u00dcT\u00dcPHANELERI \u0130\u00c7E AKTARMA\n",
        "\n",
        "# Temel Python k\u00fct\u00fcphaneleri\n",
        "import os\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Hugging Face Transformers\n",
        "from transformers import (\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer,\n",
        "    BertModel,\n",
        "    BertTokenizer,\n",
        "    pipeline,\n",
        "    AutoTokenizer,\n",
        "    AutoModel\n",
        ")\n",
        "\n",
        "# Gradio aray\u00fcz\u00fc\n",
        "import gradio as gr\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSp6vqNob4OR"
      },
      "source": [
        "## \ud83e\uddf0 **Kullan\u0131lan K\u00fct\u00fcphane ve Ara\u00e7lar \u00d6zeti**\n",
        "\n",
        "### \ud83d\udcc1 **Temel Python K\u00fct\u00fcphaneleri**\n",
        "\n",
        "* \ud83d\uddc2\ufe0f **os** \u2192 Dosya ve dizin i\u015flemleri (klas\u00f6r kontrol\u00fc, yol y\u00f6netimi vs.)\n",
        "* \ud83d\udcca **pandas** \u2192 Veri analizi ve veri \u00e7er\u00e7eveleriyle \u00e7al\u0131\u015fma (DataFrame)\n",
        "* \ud83d\udd22 **numpy** \u2192 Say\u0131sal hesaplamalar ve dizi i\u015flemleri\n",
        "* \ud83d\udcc8 **tqdm** \u2192 D\u00f6ng\u00fclerde ilerleme \u00e7ubu\u011fu g\u00f6sterimi (progress bar)\n",
        "* \ud83c\udfa8 **matplotlib & seaborn** \u2192 Statik veri g\u00f6rselle\u015ftirmeleri\n",
        "* \ud83c\udf10 **plotly** \u2192 Etkile\u015fimli ve dinamik grafikler\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udd25 **PyTorch (Derin \u00d6\u011frenme)**\n",
        "\n",
        "* \ud83d\udca1 **torch** \u2192 Temel derin \u00f6\u011frenme yap\u0131lar\u0131 ve tensor i\u015flemleri\n",
        "* \ud83e\udde0 **torch.nn.functional** \u2192 Aktivasyon fonksiyonlar\u0131 ve kay\u0131p hesaplamalar\u0131\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83e\udd17 **Hugging Face Transformers (NLP)**\n",
        "\n",
        "* \ud83e\udde0 **GPT2LMHeadModel, GPT2Tokenizer** \u2192 GPT-2 metin \u00fcretimi ve tokenizasyonu\n",
        "* \ud83d\udd0d **BertModel, BertTokenizer** \u2192 BERT metin analizi ve tokenizasyonu\n",
        "* \ud83d\udd27 **pipeline** \u2192 Kolayca haz\u0131r NLP i\u015flemleri (text-generation, sentiment vs.)\n",
        "* \ud83e\udde9 **AutoTokenizer, AutoModel** \u2192 Otomatik model/tokenizer y\u00fckleme\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83c\udf10 **Gradio (Web Aray\u00fcz\u00fc)**\n",
        "\n",
        "* \ud83d\udda5\ufe0f **gr (Gradio)** \u2192 Web tabanl\u0131 demo ve aray\u00fczler olu\u015fturmak i\u00e7in basit ara\u00e7\n",
        "  \u27a1\ufe0f Model \u00e7\u0131kt\u0131lar\u0131n\u0131n kullan\u0131c\u0131yla kolayca etkile\u015fimli payla\u015f\u0131m\u0131 \ud83e\udd1d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 63,
          "status": "ok",
          "timestamp": 1752824996603,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "pERLWF9egt0C",
        "outputId": "ab316288-8e30-4b5e-f2e0-8e1aae01df21"
      },
      "outputs": [],
      "source": [
        "# GPU kontrol ve ayarlar\u0131\n",
        "print(\"\ud83d\udd27 Sistem Kontrolleri:\")\n",
        "print(f\"\u2705 CUDA Mevcut: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\ud83d\ude80 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"\ud83d\udcca GPU Bellek: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f  GPU bulunamad\u0131, CPU kullan\u0131lacak\")\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(f\"\ud83c\udfaf Kullan\u0131lacak Cihaz: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrJU2MajgwMQ"
      },
      "source": [
        "CPU: Genel ama\u00e7l\u0131, s\u0131ral\u0131 i\u015flemler i\u00e7in g\u00fc\u00e7l\u00fc, esnek ama daha yava\u015f.\n",
        "\n",
        "GPU: Paralel i\u015flemler i\u00e7in optimize, b\u00fcy\u00fck veri ve derin \u00f6\u011frenmede \u00e7ok daha h\u0131zl\u0131. GPU, makine \u00f6\u011frenimi gibi yo\u011fun hesaplama gerektiren g\u00f6revlerde tercih edilirken, CPU genel sistem y\u00f6netimi i\u00e7in vazge\u00e7ilmezdir.\n",
        "\n",
        "Kod, sistemde GPU varsa onu kullanmay\u0131 tercih ediyor \u00e7\u00fcnk\u00fc GPU, derin \u00f6\u011frenme ve yo\u011fun hesaplamalarda daha h\u0131zl\u0131. GPU yoksa, CPU otomatik olarak se\u00e7iliyor. Bu, hem performans optimizasyonu hem de sistem uyumlulu\u011fu i\u00e7in yap\u0131lan bir kontrol.\n",
        "\n",
        "\n",
        "[ ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkHHXwOhgw52"
      },
      "outputs": [],
      "source": [
        "# Matplotlib ayarlar\u0131\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Seaborn ayarlar\u0131\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Pandas ayarlar\u0131\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "# Numpy ayarlar\u0131\n",
        "np.random.seed(42)\n",
        "\n",
        "# PyTorch ayarlar\u0131\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUOI0hwtg0dy"
      },
      "source": [
        "G\u00f6rselle\u015ftirme: Matplotlib ve Seaborn ayarlar\u0131, grafiklerin daha okunabilir ve estetik olmas\u0131n\u0131 sa\u011flar.\n",
        "\n",
        "Veri G\u00f6r\u00fcnt\u00fcleme: Pandas ayarlar\u0131, veri \u00e7er\u00e7evelerinin konsolda daha iyi g\u00f6r\u00fcnt\u00fclenmesini sa\u011flar.\n",
        "\n",
        "Tekrarlanabilirlik: Numpy ve PyTorch'ta rastgele say\u0131 \u00fcrete\u00e7lerine sabit bir ba\u015flang\u0131\u00e7 de\u011feri (seed=42) atanmas\u0131, deneylerin ve sonu\u00e7lar\u0131n tekrarlanabilir olmas\u0131n\u0131 sa\u011flar. Bu, \u00f6zellikle makine \u00f6\u011frenimi modellerinde \u00f6nemlidir, \u00e7\u00fcnk\u00fc rastgelelik sonu\u00e7lar\u0131 etkileyebilir.\n",
        "\n",
        "GPU Deste\u011fi: PyTorch ayarlar\u0131, GPU varsa onun da tekrarlanabilir olmas\u0131n\u0131 sa\u011flar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f93-ViuPgzA5"
      },
      "outputs": [],
      "source": [
        "# Genel yard\u0131mc\u0131 fonksiyonlar\n",
        "def print_separator(title=\"\"):\n",
        "    \"\"\"G\u00fczel bir ay\u0131r\u0131c\u0131 yazd\u0131r\u0131r\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    if title:\n",
        "        print(f\"\ud83c\udfaf {title}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "def check_model_size(model):\n",
        "    \"\"\"Model boyutunu hesaplar\"\"\"\n",
        "    param_count = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"\ud83d\udcca Model Parametresi: {param_count:,}\")\n",
        "    print(f\"\ud83d\udce6 Tahmini Boyut: {param_count * 4 / (1024**2):.1f} MB\")\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"GPU belle\u011fini temizler\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"\ud83e\uddf9 GPU belle\u011fi temizlendi\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41_Dy-Arg5ay"
      },
      "source": [
        "Bu fonksiyonlar, makine \u00f6\u011frenimi projelerinde\n",
        "\n",
        "kodun okunabilirli\u011fini art\u0131rmak (print_separator),\n",
        "\n",
        "model boyutunu analiz etmek (check_model_size) ve\n",
        "\n",
        "GPU kaynaklar\u0131n\u0131 verimli kullanmak (clear_gpu_memory)\n",
        "\n",
        "i\u00e7in yard\u0131mc\u0131 ara\u00e7lar sa\u011flar. \u00d6zellikle derin \u00f6\u011frenme \u00e7al\u0131\u015fmalar\u0131nda, bu t\u00fcr fonksiyonlar hata ay\u0131klamay\u0131 ve kaynak y\u00f6netimini kolayla\u015ft\u0131r\u0131r."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1752825037449,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "qDyvCnjZg26P",
        "outputId": "49467740-1238-4fcd-e552-d13cd085c2d5"
      },
      "outputs": [],
      "source": [
        "print_separator(\"LLM'lere Giri\u015f ve Temel Kavramlar\")\n",
        "print(\"\ud83d\ude80 Proje ba\u015flat\u0131l\u0131yor...\")\n",
        "print(\"\ud83d\udcda K\u00fct\u00fcphaneler y\u00fcklendi\")\n",
        "print(\"\u26a1 GPU kontrol\u00fc tamamland\u0131\")\n",
        "print(\"\ud83c\udfaf Haz\u0131r!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYXSL4KnhA2Q"
      },
      "source": [
        "# **2- MODEL Y\u00dcKLEME (GPT-2 VE BERT)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snhs_Q31hFqO"
      },
      "source": [
        "\n",
        "## \ud83d\udd0d Transformer Mimarisi ve NLP Modelleri\n",
        "\n",
        "### \ud83e\udde0 Transformer Mimarisi\n",
        "\n",
        "Transformer, do\u011fal dil i\u015fleme (NLP) ve derin \u00f6\u011frenme alanlar\u0131nda kullan\u0131lan modern bir yapay sinir a\u011f\u0131 yap\u0131s\u0131d\u0131r. \u00d6zellikle **BERT**, **GPT** gibi dil modelleri ve \u00e7eviri sistemlerinde yayg\u0131n olarak kullan\u0131l\u0131r.\n",
        "Geleneksel s\u0131ral\u0131 modeller (RNN, LSTM) yerine **paralel i\u015flem** yapabildi\u011fi i\u00e7in daha h\u0131zl\u0131 ve verimlidir.\n",
        "\n",
        "### \ud83c\udfaf Attention Mekanizmas\u0131\n",
        "\n",
        "Transformer'\u0131n temel bile\u015fenidir.\n",
        "Her kelimenin di\u011fer kelimelerle olan ili\u015fkisini de\u011ferlendirerek \u00f6nemli olanlara daha fazla **a\u011f\u0131rl\u0131k (dikkat)** verir.\n",
        "\n",
        "**\u00d6rnek:**\n",
        "*\"Kedi a\u011faca t\u0131rmand\u0131.\"* c\u00fcmlesinde, \u201ckedi\u201d ve \u201ct\u0131rmand\u0131\u201d kelimeleri anlam a\u00e7\u0131s\u0131ndan ili\u015fkilidir ve model bu ba\u011f\u0131 fark eder.\n",
        "Attention sayesinde model, ba\u011flam\u0131 daha iyi anlar.\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udd0d NLP ve Dil Modelleri: Temel Bilgiler\n",
        "\n",
        "| **Model**      | **Y\u00f6n\u00fc**           | **Ne Yapar?**                         | **Kullan\u0131m Alan\u0131**                   |\n",
        "| -------------- | ------------------ | ------------------------------------- | ------------------------------------ |\n",
        "| **RNN / LSTM** | S\u0131ral\u0131 (tek y\u00f6nl\u00fc) | Kelimeleri s\u0131rayla i\u015fler              | Eski modeller, \u00e7eviri, duygu analizi |\n",
        "| **Seq2Seq**    | Encoder + Decoder  | Girdi dizisini \u00e7\u0131kt\u0131 dizisine \u00e7evirir | Makine \u00e7evirisi, \u00f6zetleme            |\n",
        "| **BERT**       | \u00c7ift y\u00f6nl\u00fc         | Anlam \u00e7\u0131kar\u0131m\u0131 yapar, bo\u015fluk doldurur | Anlama, s\u0131n\u0131fland\u0131rma                |\n",
        "| **GPT**        | Tek y\u00f6nl\u00fc          | Metin \u00fcretir                          | Yaz\u0131 yazma, sohbet, kod \u00fcretimi      |\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83c\udfd7\ufe0f Encoder - Decoder Yap\u0131s\u0131\n",
        "\n",
        "* **Encoder**: Girdi metnini i\u015fler, anlamland\u0131r\u0131r.\n",
        "* **Decoder**: \u0130\u015flenen bilgiden yeni \u00e7\u0131kt\u0131 \u00fcretir.\n",
        "\n",
        "| Yap\u0131 T\u00fcr\u00fc       | A\u00e7\u0131klama              | \u00d6rnek Model |\n",
        "| --------------- | --------------------- | ----------- |\n",
        "| Encoder-Only    | Sadece anlama         | BERT        |\n",
        "| Decoder-Only    | Sadece \u00fcretme         | GPT         |\n",
        "| Encoder-Decoder | Hem anlama hem \u00fcretme | T5, Seq2Seq |\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83e\uddea E\u011fitim A\u015famalar\u0131\n",
        "\n",
        "#### \ud83d\udd27 Pre-training (\u00d6n E\u011fitim)\n",
        "\n",
        "* Model, \u00e7ok b\u00fcy\u00fck metinlerle e\u011fitilir.\n",
        "* Ama\u00e7: Genel dil kurallar\u0131n\u0131 \u00f6\u011frenmek (gramer, yap\u0131, ba\u011flam).\n",
        "* **\u00d6rnek veriler:**\n",
        "\n",
        "  * \"Ali okula gitti.\"\n",
        "  * \"Bug\u00fcn hava \u00e7ok g\u00fczel.\"\n",
        "  * \"Kediler s\u00fct i\u00e7er.\"\n",
        "* Hen\u00fcz belirli bir g\u00f6rev yoktur.\n",
        "\n",
        "#### \ud83c\udfaf Fine-tuning (\u0130nce Ayar / G\u00f6reve Uyarlama)\n",
        "\n",
        "* Model, belirli bir g\u00f6rev i\u00e7in \u00f6zel verilerle tekrar e\u011fitilir.\n",
        "* **\u00d6rnek:**\n",
        "  E-posta spam filtresi olu\u015fturulmak istenirse, modele \u201cspam\u201d ve \u201cde\u011fil\u201d etiketli e-postalar g\u00f6sterilir.\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udee0\ufe0f Teknik Ara\u00e7lar\n",
        "\n",
        "* **Hugging Face**: Haz\u0131r NLP modelleri k\u00fct\u00fcphanesi\n",
        "* **PyTorch**: Derin \u00f6\u011frenme framework\u2019\u00fc\n",
        "* **Gradio**: Basit web aray\u00fcz\u00fc olu\u015fturma arac\u0131\n",
        "* **GPU**: H\u0131zl\u0131 hesaplama i\u00e7in grafik i\u015flem birimi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556,
          "referenced_widgets": [
            "0f93243d6ab74f229880c9b09d69ea9b",
            "21859813170047c6a7ec3048bafb2096",
            "c8e24c3bcad040ce8f4b9271562b56d7",
            "c20660e9c53d4921a15e12165f3a754f",
            "55fa221c1a164b2a8b34d7e5f951b57d",
            "50996f5ed9d646949e0047aba38ee735",
            "d65660f1db65428d9130bbda53556c66",
            "0c26c29f6f37469189ee68fa677b74ed",
            "71a9bfb6b81547d78148cf8db3cbe9c2",
            "30b49ca6ebf74c76a5db82c179258dc3",
            "7108ee53614a42c3ab35468d51a03dc0",
            "123317d71fff428a98bbcb3bb1cab1e2",
            "1197ddad985d4b29ac2f860a586a4c61",
            "0c64b00ab2ce4cecb672d33f78d78322",
            "d24523c9d12c4bcfb48a5795aaf73340",
            "4711435d06674209a564b3b8c202bf69",
            "1828df0a49de4193ad344d53cb4f296a",
            "492df984157d45ca916aeeec3095daee",
            "f0e2a5abf78d420ab064e2bd79b74dde",
            "79a2b334589a40e5ab5d9baf5c39d057",
            "7d53801e29dd41a4a86efaa645f3152a",
            "e56ec93ebea5427fa9d7a2ae80650e30",
            "0d9d0c6c494f423494b9ea05dc0792f2",
            "215beb9747d648fca165317a6ea7a356",
            "5e409c7b401e4f12a4a1bea0ba253056",
            "617976a669f945d8b07a48b4902faa89",
            "5487b67b1ae6454ebc975640279f7abc",
            "841ea6d7d93b4f03855b7f990f84d0c6",
            "be77f917363740d5ac2bd59ff0136f00",
            "dd683d24e6854b6bb4d6d42157c51d29",
            "1de80f2b90e4478f8c405492eead4943",
            "df78c9df6b884506a189bed41772ddcf",
            "a7a5f90206d54cb5aa7f0cd0b24edaf2",
            "b2d80b7221c042169348f8ec0a7c312c",
            "bfaa20bda6254506a7d33ae732b071a9",
            "c4f0b0236e024da5b40d2b0b2962df87",
            "3c051b68544c401695f1657010e9c103",
            "d676e6c04bc643668a15c3d10a94746a",
            "805f306d4776465293046d9f7d126ac4",
            "7ed2457526df4fb28f590efdfb83f03a",
            "4ba0b2164841435599b783d1b78da714",
            "9dce55580fa7474b900ba22cda35ae4d",
            "5ed4a27f0973462286d18709f69a85f2",
            "53e12f5cd58049be80760c8327874582",
            "bff91bbe3ddf48ce85537e1407f7ce4a",
            "f7349a0bfea64887bf6358cf06f08631",
            "224035a8052e4234b54f5f1901a005d7",
            "8c888b1c341942e3834d4abadb2327a9",
            "42548fb5cb6142f3b54a124c7b85b8c4",
            "0cffb7444aed455eb778ed275ec648e9",
            "36bc29cc1eaa4eb5989ecf847827df47",
            "cdb4a2e572824ea09d8cedb68facbc2c",
            "fe1d06bc41b7468a8ed3ef98cf9522af",
            "073c55e64cdc46dab8164a26e4efca31",
            "d6bde0e74f294f0caec98c899e8a8023",
            "48dfa2c8231c46e2b6d5158e777ba020",
            "443f1a2fa1a64ab0b32a6e62badc541b",
            "219e63e8b1db4dd097a1f8b5f63b2602",
            "ab8342d3a76c4ce49b12057e9fc3137d",
            "b74552f4d7464dfb864b5248a37d750f",
            "9a8b40b0b29d40698ff49e26dfb14b86",
            "427738b8368f40d5af4afafb89cd446b",
            "ada1b5eaacc44952bacc49912e6a203f",
            "d3f595e871b740439ebf1fdb1b9c7e70",
            "c8cbe84739ef4af9ad188366e6f62971",
            "79f63b2096224d17aec75df3feb36cb0",
            "a0bf2f2973de41618d424444e2935047",
            "e4455a4862a144ae9a30059bd85f40c1",
            "701083eaa0074f5186373eb619e7379f",
            "4389bc99a666475e9af36d555e6aa571",
            "8d64662d1d1c4faa975b65832275c386",
            "64422e65811f489d9665158db4165593",
            "a7e4814b16e4439d841226fb8cc000a2",
            "c238f0d643314822a9ce06c5591a2f67",
            "7fca26c0cfba466cbe258e37e7a8e4ca",
            "56e26daad245400dafaf4c2b588cc7ba",
            "d2bd0a224a5248c181f00cb379e4cdb6"
          ]
        },
        "executionInfo": {
          "elapsed": 12453,
          "status": "ok",
          "timestamp": 1752825115377,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "vqGvhOe1hGvQ",
        "outputId": "42329e25-987d-433c-879d-134a1ed1ab04"
      },
      "outputs": [],
      "source": [
        "#GPT-2 Model Y\u00fckleme\n",
        "\n",
        "print_separator(\"GPT-2 Model Y\u00fckleme\")\n",
        "\n",
        "try:\n",
        "    # GPT-2 tokenizer ve model y\u00fckleme\n",
        "    print(\"\ud83d\udce5 GPT-2 tokenizer y\u00fckleniyor...\")\n",
        "    gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "    # Pad token ekle (GPT-2'de yoktur)\n",
        "    gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
        "\n",
        "    print(\"\ud83d\udce5 GPT-2 model y\u00fckleniyor...\")\n",
        "    gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "    # Modeli GPU'ya ta\u015f\u0131\n",
        "    gpt2_model = gpt2_model.to(device)\n",
        "    gpt2_model.eval()  # Evaluation moduna al\n",
        "\n",
        "    print(\"\u2705 GPT-2 ba\u015far\u0131yla y\u00fcklendi!\")\n",
        "    check_model_size(gpt2_model)\n",
        "\n",
        "    # Test\n",
        "    test_input = \"The future of artificial intelligence\"\n",
        "    test_tokens = gpt2_tokenizer.encode(test_input, return_tensors='pt').to(device)\n",
        "    print(f\"\ud83e\uddea Test input: '{test_input}'\")\n",
        "    print(f\"\ud83d\udd22 Token say\u0131s\u0131: {test_tokens.shape[1]}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\u274c GPT-2 y\u00fckleme hatas\u0131: {e}\")\n",
        "    gpt2_model = None\n",
        "    gpt2_tokenizer = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZUhvDjlhKXC"
      },
      "source": [
        "\n",
        "## \ud83d\udd24 Tokenizer\n",
        "\n",
        "Metni, modelin anlayabilece\u011fi say\u0131sal forma \u00e7evirir.\n",
        "**GPT-2**, kelimeleri veya kelime par\u00e7alar\u0131n\u0131 **token**'lara b\u00f6ler.\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83e\udd16 Model\n",
        "\n",
        "**GPT-2**, verilen metni okuyup **devam\u0131n\u0131 tahmin edebilir**.\n",
        "\u00d6rne\u011fin:\n",
        "Girdi: `\"Merhaba, ben\"`\n",
        "\u00c7\u0131kt\u0131: `\"Merhaba, ben bir yapay zeka modeliyim.\"`\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udccc GPT-2 Nedir?\n",
        "\n",
        "**GPT-2 (Generative Pre-trained Transformer 2)**, OpenAI taraf\u0131ndan geli\u015ftirilen **Transformer tabanl\u0131** bir dil modelidir.\n",
        "\n",
        "### \ud83d\udd39 Temel \u00d6zellikleri:\n",
        "\n",
        "* **Ama\u00e7:**\n",
        "  Metin \u00fcretimi, metin tamamlama, \u00e7eviri gibi do\u011fal dil i\u015fleme g\u00f6revlerinde kullan\u0131l\u0131r.\n",
        "\n",
        "* **Mimari:**\n",
        "  Sadece **Transformer\u2019\u0131n decoder k\u0131sm\u0131n\u0131** kullan\u0131r.\n",
        "  Her kelimeyi \u00f6nceki kelimelere bakarak tahmin eder (otoregresif model).\n",
        "\n",
        "* **\u00d6n E\u011fitim:**\n",
        "  \u00c7ok b\u00fcy\u00fck metin veri setleriyle (\u00f6rne\u011fin: web sayfalar\u0131, kitaplar) e\u011fitilmi\u015ftir.\n",
        "  Bu sayede genel dil bilgisini ta\u015f\u0131r.\n",
        "\n",
        "* **Boyutlar:**\n",
        "  GPT-2'nin farkl\u0131 versiyonlar\u0131 vard\u0131r:\n",
        "\n",
        "  * 124M\n",
        "  * 355M\n",
        "  * 774M\n",
        "  * 1.5B parametre\n",
        "    Kodda kullan\u0131lan `'gpt2'`, en k\u00fc\u00e7\u00fck olan\u0131d\u0131r (124 milyon parametre).\n",
        "\n",
        "* **Kullan\u0131m Alanlar\u0131:**\n",
        "\n",
        "  * Sohbet botlar\u0131\n",
        "  * Hik\u00e2ye yaz\u0131m\u0131\n",
        "  * Metin \u00f6zetleme\n",
        "  * Soru-cevap sistemleri\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368,
          "referenced_widgets": [
            "a2d3ee3fbcca4152bb69ab1ab1395a7e",
            "162039cb478e4954bbcbe80e792d598a",
            "4e5d69138c734af79e82ac29bae78557",
            "d951e487c71f4780a943350f73c464cf",
            "55edc68a810c46ef8c7cb01075e349c4",
            "f461924e9d0e49169dbc19ff949f6808",
            "357fe18d3d33486295759efb061d9bd7",
            "da96e4f21528432a9a3223cb5b378304",
            "7324d4e7fba249c8b7af6a0c34c53004",
            "f4e8f329390842db9f4d5f63f7bb7766",
            "d6a264e1cf934b5a8634c14e363b6b74",
            "83ec9b4c9e444d58b2eebf0c5dc5f400",
            "5bcbedf9e453418b957b1e5bb789f5db",
            "90a69adb6795447d8cadaefb5285417f",
            "42a4db05b7324b7e952fde05a0c21009",
            "24a9e8c91d9c43adb8ec555d5ab010b3",
            "fb06c41b3965418c92c8f43fdbf1a0ba",
            "d6055ddbaebf48b2a499ea5c09670e05",
            "4ce087ab5b8e4fb4b5ca1a60f8390c5a",
            "eab7b37f019e42a5a92079ef51f0b4e5",
            "97a9f707aea449128675152836b5dcac",
            "46b2a8fe68694f69a8e2d399be7e4efd",
            "897f1b2367cc477c9ab43cdac01b3d26",
            "f23836ad491448c2a0ae7f108a1bc65b",
            "ee14b2f941904fdb8e75dd85930d5295",
            "8d1f330e8fad4b5fa634dae4384c9e66",
            "73296ae0bcf541e0bcdfa1d891cb4f1c",
            "2784c243b74f429bbd3e8b3b789a0152",
            "84328bef20694d49971ec6e18899f23b",
            "e9b7a8778b3b4e2491e4ec1c570f3d43",
            "c05a03f783604aa690d2d99e16f0a2c0",
            "cc56aae90e7242ea80b2509a4ee56f73",
            "18eb03235ce74cf98c5a220dc1a6c4e0",
            "16667503fb37493998bd9bacc6a5a57a",
            "9e192170ae6249f18a93de9be45ad616",
            "87db5bec695349faaf88a9119c024aa8",
            "cb32a4a8b401497fa222d17c5e98ae57",
            "d37445a08be3485e8c82274a822b7b06",
            "5e19d68b730e4762a5500b2257a8b3f9",
            "43744dbfc8b44a099c6b041d2de280f5",
            "3825ff6d605048b1b62ee196466d334e",
            "4464960981bf42eeba02d3512d46bde3",
            "ffbb76c37d694c078aafef55b8703957",
            "c120c8c3c3354176834c408e651a0bd9",
            "79e820b104194496bf8030d992a1d0aa",
            "9ec219d88cb34555b0e5b34aa01e264a",
            "eeae6d41cfe9461a8ea516f402363026",
            "d92a1fed253a4c50a7cb65885a5e4cf9",
            "1eac815661ea4853a3b15d1cdc281287",
            "18eaec06e7cc43318a3261a04d7e3ce9",
            "91d4e3b7b90c4a13ac0dc907b0c328e4",
            "e854a8c8673c44b49a5c8dc589dd4e84",
            "3f99d778a90e4c03a5a26c296cb21352",
            "102dff961a2f4cfa9b6aacd772d7f764",
            "630e76464c6f41ce8b1e562026efffcc"
          ]
        },
        "executionInfo": {
          "elapsed": 8442,
          "status": "ok",
          "timestamp": 1752825135455,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "oGPc2u1ShMp6",
        "outputId": "8ecb4b99-81b3-4137-8236-7ef6b0948ec1"
      },
      "outputs": [],
      "source": [
        "#BERT Model Y\u00fckleme^\n",
        "\n",
        "print_separator(\"BERT Model Y\u00fckleme\")\n",
        "\n",
        "try:\n",
        "    # BERT tokenizer ve model y\u00fckleme\n",
        "    print(\"\ud83d\udce5 BERT tokenizer y\u00fckleniyor...\")\n",
        "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    print(\"\ud83d\udce5 BERT model y\u00fckleniyor...\")\n",
        "    bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Modeli GPU'ya ta\u015f\u0131\n",
        "    bert_model = bert_model.to(device)\n",
        "    bert_model.eval()  # Evaluation moduna al\n",
        "\n",
        "    print(\"\u2705 BERT ba\u015far\u0131yla y\u00fcklendi!\")\n",
        "    check_model_size(bert_model)\n",
        "\n",
        "    # Test\n",
        "    test_input = \"The future of artificial intelligence\"\n",
        "    test_tokens = bert_tokenizer.encode(test_input, return_tensors='pt').to(device)\n",
        "    print(f\"\ud83e\uddea Test input: '{test_input}'\")\n",
        "    print(f\"\ud83d\udd22 Token say\u0131s\u0131: {test_tokens.shape[1]}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\u274c BERT y\u00fckleme hatas\u0131: {e}\")\n",
        "    bert_model = None\n",
        "    bert_tokenizer = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxlrewFThObr"
      },
      "source": [
        "## \u2702\ufe0f 3. Tokenization (Token\u2019lara Ay\u0131rma)\n",
        "\n",
        "* Metinleri, modelin anlayabilece\u011fi k\u00fc\u00e7\u00fck say\u0131sal par\u00e7alara b\u00f6ler.\n",
        "* Bu par\u00e7alara **token** denir (kelime, kelime par\u00e7as\u0131 veya harf olabilir).\n",
        "\n",
        "\ud83c\udfaf **\u00d6rnek:**\n",
        "Metin: `\"Kitap okuyorum\"`\n",
        "Token'lara ayr\u0131labilir: `\"Ki\"`, `\"tap\"`, `\"oku\"`, `\"yor\"`, `\"um\"` gibi k\u00fc\u00e7\u00fck par\u00e7alar\n",
        "\u27a1\ufe0f Bu token\u2019lar daha sonra say\u0131lara \u00e7evrilir.\n",
        "\n",
        "\ud83e\udde0 **D\u00fc\u015f\u00fcn:** Bir c\u00fcmleyi LEGO par\u00e7alar\u0131na ay\u0131rmak gibi.\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udd22 4. Embeddings (Say\u0131sal Temsil)\n",
        "\n",
        "* Her kelime/token, modelde anlam\u0131n\u0131 ta\u015f\u0131yan bir say\u0131 vekt\u00f6r\u00fcne d\u00f6n\u00fc\u015ft\u00fcr\u00fcl\u00fcr.\n",
        "* Bu sayede model, kelimeler aras\u0131ndaki anlam ili\u015fkilerini **say\u0131sal olarak** anlayabilir.\n",
        "\n",
        "\ud83c\udfaf **\u00d6rnek:**\n",
        "\n",
        "* `\"kitap\"` \u2192 `[0.12, -0.45, 0.87, ...]`\n",
        "* `\"defter\"` \u2192 `[0.14, -0.43, 0.85, ...]`\n",
        "  \u27a1\ufe0f Bu iki kelimenin say\u0131 vekt\u00f6rleri benzer oldu\u011fu i\u00e7in model, anlamlar\u0131n\u0131n yak\u0131n oldu\u011funu bilir.\n",
        "\n",
        "\ud83e\udde0 **D\u00fc\u015f\u00fcn:** Kelimeleri uzaydaki noktalar gibi d\u00fc\u015f\u00fcn \u2014 yak\u0131n olanlar anlamca benzer.\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udcd8 BERT\n",
        "\n",
        "**Tan\u0131m:**\n",
        "Google\u2019\u0131n 2018\u2019de geli\u015ftirdi\u011fi, Transformer tabanl\u0131, **\u00e7ift y\u00f6nl\u00fc** dil modeli.\n",
        "\n",
        "**Ama\u00e7:**\n",
        "Metinlerin anlam\u0131n\u0131 \u00e7\u0131karmak (s\u0131n\u0131fland\u0131rma, soru-cevap, varl\u0131k tan\u0131ma).\n",
        "\n",
        "**\u00c7al\u0131\u015fma \u015eekli:**\n",
        "\n",
        "* **\u00c7ift y\u00f6nl\u00fc:** C\u00fcmlenin tamam\u0131n\u0131 ayn\u0131 anda analiz eder.\n",
        "\n",
        "**\u00d6n E\u011fitim:**\n",
        "\n",
        "* **MLM (Masked Language Model):** Rastgele kelimeler gizlenir ve modelden bunlar\u0131 tahmin etmesi beklenir.\n",
        "* **NSP (Next Sentence Prediction):** \u0130ki c\u00fcmle aras\u0131nda ba\u011flant\u0131 olup olmad\u0131\u011f\u0131n\u0131 tahmin eder.\n",
        "\n",
        "**\u0130nce Ayar (Fine-tuning):**\n",
        "\n",
        "* \u00d6zel g\u00f6revler i\u00e7in model yeniden e\u011fitilir.\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udd27 BERT\u2019in Bile\u015fenleri\n",
        "\n",
        "* **Tokenizer:**\n",
        "  WordPiece algoritmas\u0131yla metni token\u2019lara b\u00f6ler. \u00d6zel token\u2019lar: `[CLS]`, `[SEP]`\n",
        "\n",
        "### \ud83d\udee0\ufe0f Kullan\u0131m Alanlar\u0131\n",
        "\n",
        "* Duygu analizi\n",
        "* Soru-cevap sistemleri\n",
        "* Arama motorlar\u0131 (\u00f6rne\u011fin Google)\n",
        "* Adland\u0131r\u0131lm\u0131\u015f varl\u0131k tan\u0131ma (NER)\n",
        "\n",
        "---\n",
        "\n",
        "### \u2705 Avantaj\n",
        "\n",
        "* Ba\u011flam\u0131 **derinlemesine** anlar.\n",
        "\n",
        "### \u26a0\ufe0f Dezavantaj\n",
        "\n",
        "* Y\u00fcksek **hesaplama g\u00fcc\u00fc** gerektirir.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 21596,
          "status": "ok",
          "timestamp": 1752825166446,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "NsEwcTz3hRMD",
        "outputId": "362c2fd5-102c-4d80-80d2-31a3264892d2"
      },
      "outputs": [],
      "source": [
        "#Pipeline Y\u00fckleme (Alternatif Y\u00f6ntem)\n",
        "\n",
        "print_separator(\"Pipeline Y\u00fckleme\")\n",
        "\n",
        "try:\n",
        "    # GPT-2 pipeline (metin \u00fcretimi i\u00e7in)\n",
        "    print(\"\ud83d\udce5 GPT-2 pipeline y\u00fckleniyor...\")\n",
        "    gpt2_pipeline = pipeline(\n",
        "        'text-generation',\n",
        "        model='gpt2',\n",
        "        tokenizer='gpt2',\n",
        "        device=0 if torch.cuda.is_available() else -1\n",
        "    )\n",
        "\n",
        "    # BERT pipeline (feature extraction i\u00e7in)\n",
        "    print(\"\ud83d\udce5 BERT pipeline y\u00fckleniyor...\")\n",
        "    bert_pipeline = pipeline(\n",
        "        'feature-extraction',\n",
        "        model='bert-base-uncased',\n",
        "        tokenizer='bert-base-uncased',\n",
        "        device=0 if torch.cuda.is_available() else -1\n",
        "    )\n",
        "\n",
        "    print(\"\u2705 Pipeline'lar ba\u015far\u0131yla y\u00fcklendi!\")\n",
        "\n",
        "    # Pipeline test\n",
        "    test_text = \"Artificial intelligence is\"\n",
        "    gpt2_result = gpt2_pipeline(test_text, max_length=20, num_return_sequences=1)\n",
        "    print(f\"\ud83e\uddea GPT-2 Pipeline Test: {gpt2_result[0]['generated_text']}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\u274c Pipeline y\u00fckleme hatas\u0131: {e}\")\n",
        "    gpt2_pipeline = None\n",
        "    bert_pipeline = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndzc52L-hSt-"
      },
      "source": [
        "## \ud83d\udd04 Pipeline Nedir ve Ne \u0130\u015fe Yarar?\n",
        "\n",
        "### \ud83d\udccc Pipeline\n",
        "\n",
        "**Pipeline**, Hugging Face\u2019in `transformers` k\u00fct\u00fcphanesinde, NLP g\u00f6revlerini kolayla\u015ft\u0131rmak i\u00e7in tasarlanm\u0131\u015f y\u00fcksek seviyeli bir API\u2019dir.\n",
        "Model ve tokenizer\u2019\u0131 otomatik olarak birle\u015ftirir ve belirli bir g\u00f6rev i\u00e7in haz\u0131r bir i\u015f ak\u0131\u015f\u0131 sunar.\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udca1 Ne \u0130\u015fe Yarar?\n",
        "\n",
        "* **Kolay Kullan\u0131m:**\n",
        "  Model ve tokenizer\u2019\u0131 ayr\u0131 ayr\u0131 yap\u0131land\u0131rmak yerine, tek sat\u0131rda bir NLP g\u00f6revi i\u00e7in haz\u0131r bir sistem olu\u015fturur.\n",
        "\n",
        "* **G\u00f6rev \u00c7e\u015fitlili\u011fi:**\n",
        "  Metin \u00fcretimi, \u00f6zellik \u00e7\u0131karma, duygu analizi, \u00e7eviri gibi bir\u00e7ok NLP g\u00f6revini destekler.\n",
        "\n",
        "* **H\u0131zl\u0131 Prototipleme:**\n",
        "  Ara\u015ft\u0131rmac\u0131lar ve geli\u015ftiriciler, karma\u015f\u0131k kodlar yazmadan modelleri h\u0131zl\u0131ca test edebilir.\n",
        "\n",
        "* **Cihaz Deste\u011fi:**\n",
        "  GPU veya CPU\u2019yu otomatik olarak alg\u0131lay\u0131p kullanabilir.\n",
        "\n",
        "---\n",
        "\n",
        "### \u2699\ufe0f A\u00e7\u0131klama\n",
        "\n",
        "Bu kod, **GPT-2** ve **BERT** modellerini `pipeline` API\u2019si ile y\u00fckleyerek:\n",
        "\n",
        "* **Metin \u00fcretimi** (GPT-2)\n",
        "* **\u00d6zellik \u00e7\u0131karma** (BERT)\n",
        "\n",
        "g\u00f6revleri i\u00e7in haz\u0131r hale getiriyor.\n",
        "\n",
        "---\n",
        "\n",
        "### \u2705 Avantajlar\n",
        "\n",
        "* Pipeline, NLP g\u00f6revlerini basitle\u015ftirir.\n",
        "* H\u0131zl\u0131 prototipleme imk\u00e2n\u0131 sa\u011flar.\n",
        "* \u00c7\u0131kt\u0131lar, modellerin ba\u015far\u0131yla y\u00fcklendi\u011fini ve GPT-2\u2019nin metin \u00fcretti\u011fini g\u00f6sterir.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 19,
          "status": "ok",
          "timestamp": 1752825166466,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "6jAA1DEqhVA_",
        "outputId": "88b781a5-b5d3-4b85-8285-d9a410bc6212"
      },
      "outputs": [],
      "source": [
        "#Model Bilgileri ve Kar\u015f\u0131la\u015ft\u0131rma\n",
        "\n",
        "print_separator(\"Model Bilgileri\")\n",
        "\n",
        "# Model bilgilerini kar\u015f\u0131la\u015ft\u0131r\n",
        "model_info = {\n",
        "    'Model': ['GPT-2', 'BERT'],\n",
        "    'Tip': ['Decoder-Only', 'Encoder-Only'],\n",
        "    'Parametre Say\u0131s\u0131': ['124M', '110M'],\n",
        "    'Vocabulary Size': [50257, 30522],\n",
        "    'Max Sequence Length': [1024, 512],\n",
        "    'Kullan\u0131m Alan\u0131': ['Text Generation', 'Text Understanding']\n",
        "}\n",
        "\n",
        "df_models = pd.DataFrame(model_info)\n",
        "print(df_models.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 19,
          "status": "ok",
          "timestamp": 1752825179245,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "1wbxJQWIhacg",
        "outputId": "1bb6713a-e4fa-4275-af22-79dd77a87536"
      },
      "outputs": [],
      "source": [
        "#Bellek Durumu Kontrol\n",
        "\n",
        "print_separator(\"Bellek Durumu\")\n",
        "\n",
        "# GPU bellek kullan\u0131m\u0131\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\ud83d\udd0d GPU Bellek Kullan\u0131m\u0131:\")\n",
        "    print(f\"   Ayr\u0131lm\u0131\u015f: {torch.cuda.memory_allocated(0) / 1024**2:.1f} MB\")\n",
        "    print(f\"   \u00d6nbellekte: {torch.cuda.memory_reserved(0) / 1024**2:.1f} MB\")\n",
        "    print(f\"   Kullan\u0131labilir: {torch.cuda.get_device_properties(0).total_memory / 1024**2:.1f} MB\")\n",
        "\n",
        "# Model y\u00fckleme durumu kontrol\n",
        "models_loaded = []\n",
        "if gpt2_model is not None:\n",
        "    models_loaded.append(\"GPT-2 \u2705\")\n",
        "if bert_model is not None:\n",
        "    models_loaded.append(\"BERT \u2705\")\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Y\u00fcklenen Modeller: {', '.join(models_loaded)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 57,
          "status": "ok",
          "timestamp": 1752825185616,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "hLGGHvsrhcAq",
        "outputId": "5a119104-c06b-4519-ac9c-9f2042b7024d"
      },
      "outputs": [],
      "source": [
        "#Hata Ay\u0131klama Fonksiyonu\n",
        "\n",
        "def check_models():\n",
        "    \"\"\"Modellerin d\u00fczg\u00fcn y\u00fcklenip y\u00fcklenmedi\u011fini kontrol eder\"\"\"\n",
        "    print_separator(\"Model Kontrol\")\n",
        "\n",
        "    # GPT-2 kontrol\n",
        "    if gpt2_model is not None and gpt2_tokenizer is not None:\n",
        "        print(\"\u2705 GPT-2: Haz\u0131r\")\n",
        "        try:\n",
        "            test_input = \"Hello world\"\n",
        "            inputs = gpt2_tokenizer(test_input, return_tensors='pt').to(device)\n",
        "            with torch.no_grad():\n",
        "                outputs = gpt2_model(**inputs)\n",
        "            print(\"\u2705 GPT-2: Test ba\u015far\u0131l\u0131\")\n",
        "        except Exception as e:\n",
        "            print(f\"\u274c GPT-2: Test hatas\u0131 - {e}\")\n",
        "    else:\n",
        "        print(\"\u274c GPT-2: Y\u00fcklenmedi\")\n",
        "\n",
        "    # BERT kontrol\n",
        "    if bert_model is not None and bert_tokenizer is not None:\n",
        "        print(\"\u2705 BERT: Haz\u0131r\")\n",
        "        try:\n",
        "            test_input = \"Hello world\"\n",
        "            inputs = bert_tokenizer(test_input, return_tensors='pt').to(device)\n",
        "            with torch.no_grad():\n",
        "                outputs = bert_model(**inputs)\n",
        "            print(\"\u2705 BERT: Test ba\u015far\u0131l\u0131\")\n",
        "        except Exception as e:\n",
        "            print(f\"\u274c BERT: Test hatas\u0131 - {e}\")\n",
        "    else:\n",
        "        print(\"\u274c BERT: Y\u00fcklenmedi\")\n",
        "\n",
        "# Kontrol fonksiyonunu \u00e7al\u0131\u015ft\u0131r\n",
        "check_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cbk-U_jhd5n"
      },
      "source": [
        "# **3-METIN TAMAMLAMA FONKSIYONLARI**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 383,
          "status": "ok",
          "timestamp": 1752825213460,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "Wr5VdY8xhh4h",
        "outputId": "0e537972-4643-4651-af1d-c7f6b51e886d"
      },
      "outputs": [],
      "source": [
        "#GPT-2 Metin Tamamlama Fonksiyonu\n",
        "\n",
        "print_separator(\"GPT-2 Metin Tamamlama Fonksiyonu\")\n",
        "\n",
        "def generate_text_gpt2(prompt, max_length=50, num_return_sequences=1, temperature=0.7, top_p=0.9, do_sample=True):\n",
        "    \"\"\"\n",
        "    GPT-2 ile metin tamamlama\n",
        "\n",
        "    Args:\n",
        "        prompt: Ba\u015flang\u0131\u00e7 metni\n",
        "        max_length: Maksimum token say\u0131s\u0131\n",
        "        num_return_sequences: Ka\u00e7 farkl\u0131 sonu\u00e7 \u00fcretilece\u011fi\n",
        "        temperature: Yarat\u0131c\u0131l\u0131k seviyesi (0.1-2.0)\n",
        "        top_p: Nucleus sampling parametresi\n",
        "        do_sample: Sampling yap\u0131l\u0131p yap\u0131lmayaca\u011f\u0131\n",
        "    \"\"\"\n",
        "    if gpt2_model is None or gpt2_tokenizer is None:\n",
        "        return \"\u274c GPT-2 modeli y\u00fcklenmedi!\"\n",
        "\n",
        "    try:\n",
        "        # Input'u tokenize et\n",
        "        inputs = gpt2_tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "        # Giri\u015f uzunlu\u011funu kontrol et\n",
        "        if inputs.shape[1] > 1000:\n",
        "            return \"\u274c Giri\u015f metni \u00e7ok uzun! (Max 1000 token)\"\n",
        "\n",
        "        # Metin \u00fcret\n",
        "        with torch.no_grad():\n",
        "            outputs = gpt2_model.generate(\n",
        "                inputs,\n",
        "                max_length=max_length,\n",
        "                num_return_sequences=num_return_sequences,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p,\n",
        "                do_sample=do_sample,\n",
        "                pad_token_id=gpt2_tokenizer.eos_token_id,\n",
        "                attention_mask=torch.ones(inputs.shape).to(device)\n",
        "            )\n",
        "\n",
        "        # Sonu\u00e7lar\u0131 decode et\n",
        "        results = []\n",
        "        for i, output in enumerate(outputs):\n",
        "            generated_text = gpt2_tokenizer.decode(output, skip_special_tokens=True)\n",
        "            results.append({\n",
        "                'sequence': i + 1,\n",
        "                'text': generated_text,\n",
        "                'new_text': generated_text[len(prompt):].strip()\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"\u274c Hata: {str(e)}\"\n",
        "\n",
        "# Test fonksiyonu\n",
        "print(\"\ud83e\uddea GPT-2 fonksiyonu test ediliyor...\")\n",
        "test_prompt = \"The future of artificial intelligence is\"\n",
        "test_result = generate_text_gpt2(test_prompt, max_length=30)\n",
        "if isinstance(test_result, list):\n",
        "    print(f\"\u2705 Test ba\u015far\u0131l\u0131: {test_result[0]['text']}\")\n",
        "else:\n",
        "    print(f\"\u274c Test hatas\u0131: {test_result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WgvqNXQhktV"
      },
      "source": [
        "Fonksiyon Tan\u0131m\u0131: generate_text_gpt2 ad\u0131nda bir fonksiyon, verilen bir ba\u015flang\u0131\u00e7 metnini (prompt) tamamlamak i\u00e7in GPT-2 modelini kullan\u0131r. Parametreler:\n",
        "\n",
        "prompt: Ba\u015flang\u0131\u00e7 metni.\n",
        "\n",
        "max_length: \u00dcretilecek metnin maksimum uzunlu\u011fu (token say\u0131s\u0131).\n",
        "\n",
        "num_return_sequences: Ka\u00e7 farkl\u0131 metin \u00fcretilece\u011fi.\n",
        "\n",
        "temperature: Yarat\u0131c\u0131l\u0131k seviyesi (d\u00fc\u015f\u00fck de\u011ferler daha tahmin edilebilir, y\u00fcksek de\u011ferler daha yarat\u0131c\u0131 sonu\u00e7lar \u00fcretir).\n",
        "\n",
        "top_p: Nucleus sampling i\u00e7in olas\u0131l\u0131k s\u0131n\u0131r\u0131./Metin \u00fcretiminde rastgeleli\u011fi kontroll\u00fc \u015fekilde sa\u011flamak i\u00e7in kullan\u0131l\u0131r.\n",
        "\n",
        "do_sample: Rastgele \u00f6rnekleme yap\u0131l\u0131p yap\u0131lmayaca\u011f\u0131.\n",
        "\n",
        "\u00d6zet: Kod, GPT-2 modelini kullanarak kullan\u0131c\u0131dan al\u0131nan bir metni tamamlar ve test ama\u00e7l\u0131 bir \u00f6rnek \u00e7al\u0131\u015ft\u0131r\u0131r."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 59,
          "status": "ok",
          "timestamp": 1752825229486,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "IR1Y4vK3hl2c",
        "outputId": "2a0541b1-6655-4923-930b-44e7c64ccd8d"
      },
      "outputs": [],
      "source": [
        "#BERT Metin Analizi Fonksiyonu\n",
        "\n",
        "print_separator(\"BERT Metin Analizi Fonksiyonu\")\n",
        "\n",
        "def analyze_text_bert(text, return_attention=False):\n",
        "    \"\"\"\n",
        "    BERT ile metin analizi ve \u00f6zellik \u00e7\u0131kar\u0131m\u0131\n",
        "\n",
        "    Args:\n",
        "        text: Analiz edilecek metin\n",
        "        return_attention: Attention skorlar\u0131n\u0131 d\u00f6nd\u00fcr\u00fcp d\u00f6nd\u00fcrmeyece\u011fi\n",
        "    \"\"\"\n",
        "    if bert_model is None or bert_tokenizer is None:\n",
        "        return \"\u274c BERT modeli y\u00fcklenmedi!\"\n",
        "\n",
        "    try:\n",
        "        # Metni tokenize et\n",
        "        inputs = bert_tokenizer(\n",
        "            text,\n",
        "            return_tensors='pt',\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=512\n",
        "        ).to(device)\n",
        "\n",
        "        # Model \u00e7\u0131kt\u0131s\u0131n\u0131 al\n",
        "        with torch.no_grad():\n",
        "            outputs = bert_model(**inputs, output_attentions=return_attention)\n",
        "\n",
        "        # Son katman hidden state'leri\n",
        "        last_hidden_states = outputs.last_hidden_state\n",
        "\n",
        "        # CLS token (c\u00fcmle temsili)\n",
        "        cls_embedding = last_hidden_states[:, 0, :]\n",
        "\n",
        "        # Token embeddings\n",
        "        token_embeddings = last_hidden_states[0]\n",
        "\n",
        "        # Tokenlar\u0131 decode et\n",
        "        tokens = bert_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "        result = {\n",
        "            'tokens': tokens,\n",
        "            'token_count': len(tokens),\n",
        "            'cls_embedding': cls_embedding.cpu().numpy(),\n",
        "            'token_embeddings': token_embeddings.cpu().numpy(),\n",
        "            'embedding_dim': last_hidden_states.shape[-1]\n",
        "        }\n",
        "\n",
        "        if return_attention:\n",
        "            result['attention_weights'] = outputs.attentions\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"\u274c Hata: {str(e)}\"\n",
        "\n",
        "# Test fonksiyonu\n",
        "print(\"\ud83e\uddea BERT fonksiyonu test ediliyor...\")\n",
        "test_text = \"The future of artificial intelligence is bright\"\n",
        "test_result = analyze_text_bert(test_text)\n",
        "if isinstance(test_result, dict):\n",
        "    print(f\"\u2705 Test ba\u015far\u0131l\u0131: {test_result['token_count']} token, {test_result['embedding_dim']} boyut\")\n",
        "else:\n",
        "    print(f\"\u274c Test hatas\u0131: {test_result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73gcRcthhnhD"
      },
      "source": [
        "\u00d6zet: Kod, BERT modelini kullanarak metni tokenize eder, g\u00f6m\u00fclmeler ve tokenlar gibi \u00f6zellikler \u00e7\u0131kar\u0131r ve test ama\u00e7l\u0131 bir \u00f6rnek \u00e7al\u0131\u015ft\u0131r\u0131r.\n",
        "\n",
        "\u0130\u015flem Ak\u0131\u015f\u0131: BERT modeli ve tokenizer'\u0131n y\u00fckl\u00fc olup olmad\u0131\u011f\u0131 kontrol edilir.\n",
        "\n",
        "Metin, BERT tokenizer ile tokenize edilir (maksimum 512 token, kesme ve doldurma uygulan\u0131r).\n",
        "\n",
        "Model, tokenize edilen metni i\u015fler ve \u00e7\u0131kt\u0131lar (hidden states, attention skorlar\u0131) al\u0131n\u0131r.\n",
        "\n",
        "CLS token'\u0131n g\u00f6m\u00fclmesi (c\u00fcmle temsili) ve t\u00fcm tokenlar\u0131n g\u00f6m\u00fclmeleri (embeddings) elde edilir.\n",
        "\n",
        "Tokenlar, ID'lerden orijinal kelimelere \u00e7evrilir.\n",
        "\n",
        "Sonu\u00e7 olarak bir s\u00f6zl\u00fck d\u00f6nd\u00fcr\u00fcl\u00fcr:\n",
        "\n",
        "tokens: Kelime par\u00e7alar\u0131 (tokenlar) token_count: Ka\u00e7 tane token var? cls_embedding: C\u00fcmle \u00f6zeti olan CLS token\u2019\u0131n temsili token_embeddings: Her kelimenin say\u0131sal temsili embedding_dim: Her temsil ka\u00e7 boyutlu? attention_weights: (\u0130ste\u011fe ba\u011fl\u0131) Modelin dikkat da\u011f\u0131l\u0131m\u0131\n",
        "\n",
        "Hata Y\u00f6netimi: Model y\u00fcklenmemi\u015fse veya hata olu\u015fursa hata mesaj\u0131 d\u00f6nd\u00fcr\u00fcl\u00fcr.\n",
        "\n",
        "Test: Fonksiyon, \"The future of artificial intelligence is bright\" metniyle test edilir ve ba\u015far\u0131l\u0131ysa token say\u0131s\u0131 ile g\u00f6m\u00fclme boyutu yazd\u0131r\u0131l\u0131r.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 544,
          "status": "ok",
          "timestamp": 1752825244716,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "96EfUNd3hpiw",
        "outputId": "8afbbaa0-9119-48d5-a806-ccbd2716fd86"
      },
      "outputs": [],
      "source": [
        "#Geli\u015fmi\u015f Metin Tamamlama Fonksiyonu\n",
        "\n",
        "print_separator(\"Geli\u015fmi\u015f Metin Tamamlama\")\n",
        "\n",
        "def advanced_text_completion(prompt, model_type=\"gpt2\", **kwargs):\n",
        "    \"\"\"\n",
        "    Geli\u015fmi\u015f metin tamamlama fonksiyonu\n",
        "\n",
        "    Args:\n",
        "        prompt: Ba\u015flang\u0131\u00e7 metni\n",
        "        model_type: \"gpt2\" veya \"pipeline\"\n",
        "        **kwargs: Ek parametreler\n",
        "    \"\"\"\n",
        "\n",
        "    if model_type == \"gpt2\":\n",
        "        # Manuel GPT-2 kullan\u0131m\u0131\n",
        "        return generate_text_gpt2(prompt, **kwargs)\n",
        "\n",
        "    elif model_type == \"pipeline\":\n",
        "        # Pipeline kullan\u0131m\u0131\n",
        "        if gpt2_pipeline is None:\n",
        "            return \"\u274c GPT-2 pipeline y\u00fcklenmedi!\"\n",
        "\n",
        "        try:\n",
        "            max_length = kwargs.get('max_length', 50)\n",
        "            num_return_sequences = kwargs.get('num_return_sequences', 1)\n",
        "            temperature = kwargs.get('temperature', 0.7)\n",
        "\n",
        "            results = gpt2_pipeline(\n",
        "                prompt,\n",
        "                max_length=max_length,\n",
        "                num_return_sequences=num_return_sequences,\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                truncation=True\n",
        "            )\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"\u274c Pipeline hatas\u0131: {str(e)}\"\n",
        "\n",
        "    else:\n",
        "        return \"\u274c Ge\u00e7ersiz model tipi!\"\n",
        "\n",
        "# Test fonksiyonu\n",
        "print(\"\ud83e\uddea Geli\u015fmi\u015f fonksiyon test ediliyor...\")\n",
        "test_results = advanced_text_completion(\n",
        "    \"Machine learning is\",\n",
        "    model_type=\"gpt2\",\n",
        "    max_length=40,\n",
        "    temperature=0.8\n",
        ")\n",
        "if isinstance(test_results, list):\n",
        "    print(f\"\u2705 Test ba\u015far\u0131l\u0131: {test_results[0]['text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PWQMys4hske"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1868,
          "status": "ok",
          "timestamp": 1752825257126,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "nfcoy_2ohszN",
        "outputId": "d3aa9d31-328e-4c2d-a7d6-ee00b36c73dd"
      },
      "outputs": [],
      "source": [
        "#Metin Kar\u015f\u0131la\u015ft\u0131rma Fonksiyonu\n",
        "\n",
        "print_separator(\"Metin Kar\u015f\u0131la\u015ft\u0131rma Fonksiyonu\")\n",
        "\n",
        "def compare_models(prompt, gpt2_params=None, bert_analysis=True):\n",
        "    \"\"\"\n",
        "    GPT-2 ve BERT sonu\u00e7lar\u0131n\u0131 kar\u015f\u0131la\u015ft\u0131r\n",
        "\n",
        "    Args:\n",
        "        prompt: Giri\u015f metni\n",
        "        gpt2_params: GPT-2 parametreleri\n",
        "        bert_analysis: BERT analizi yap\u0131l\u0131p yap\u0131lmayaca\u011f\u0131\n",
        "    \"\"\"\n",
        "\n",
        "    if gpt2_params is None:\n",
        "        gpt2_params = {'max_length': 50, 'temperature': 0.7}\n",
        "\n",
        "    results = {\n",
        "        'prompt': prompt,\n",
        "        'gpt2_result': None,\n",
        "        'bert_result': None,\n",
        "        'comparison': {}\n",
        "    }\n",
        "\n",
        "    # GPT-2 sonucu\n",
        "    print(\"\ud83d\udd04 GPT-2 ile metin \u00fcretiliyor...\")\n",
        "    gpt2_result = generate_text_gpt2(prompt, **gpt2_params)\n",
        "    results['gpt2_result'] = gpt2_result\n",
        "\n",
        "    # BERT analizi\n",
        "    if bert_analysis:\n",
        "        print(\"\ud83d\udd04 BERT ile analiz yap\u0131l\u0131yor...\")\n",
        "        bert_result = analyze_text_bert(prompt)\n",
        "        results['bert_result'] = bert_result\n",
        "\n",
        "    # Kar\u015f\u0131la\u015ft\u0131rma\n",
        "    if isinstance(gpt2_result, list) and isinstance(bert_result, dict):\n",
        "        results['comparison'] = {\n",
        "            'gpt2_generated_length': len(gpt2_result[0]['new_text'].split()),\n",
        "            'bert_token_count': bert_result['token_count'],\n",
        "            'bert_embedding_dim': bert_result['embedding_dim']\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "# Test fonksiyonu\n",
        "print(\"\ud83e\uddea Kar\u015f\u0131la\u015ft\u0131rma fonksiyonu test ediliyor...\")\n",
        "comparison_test = compare_models(\"Technology has changed our lives\")\n",
        "if comparison_test['gpt2_result'] and comparison_test['bert_result']:\n",
        "    print(\"\u2705 Kar\u015f\u0131la\u015ft\u0131rma testi ba\u015far\u0131l\u0131!\")\n",
        "else:\n",
        "    print(\"\u274c Kar\u015f\u0131la\u015ft\u0131rma testi ba\u015far\u0131s\u0131z!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 878,
          "status": "ok",
          "timestamp": 1752825266106,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "pO0I2yp8hvWI",
        "outputId": "21689fde-c8b9-4882-d522-08a9559d6284"
      },
      "outputs": [],
      "source": [
        "#Toplu Metin \u0130\u015fleme Fonksiyonu\n",
        "\n",
        "\n",
        "print_separator(\"Toplu Metin \u0130\u015fleme\")\n",
        "\n",
        "def batch_text_processing(prompts, model_type=\"gpt2\", show_progress=True):\n",
        "    \"\"\"\n",
        "    Birden fazla prompt'u toplu olarak i\u015fler\n",
        "\n",
        "    Args:\n",
        "        prompts: Prompt listesi\n",
        "        model_type: Kullan\u0131lacak model\n",
        "        show_progress: \u0130lerleme \u00e7ubu\u011fu g\u00f6sterilsin mi\n",
        "    \"\"\"\n",
        "\n",
        "    results = []\n",
        "\n",
        "    iterator = tqdm(prompts, desc=\"\u0130\u015fleniyor\") if show_progress else prompts\n",
        "\n",
        "    for prompt in iterator:\n",
        "        if model_type == \"gpt2\":\n",
        "            result = generate_text_gpt2(prompt, max_length=30)\n",
        "        elif model_type == \"bert\":\n",
        "            result = analyze_text_bert(prompt)\n",
        "        else:\n",
        "            result = \"\u274c Ge\u00e7ersiz model tipi\"\n",
        "\n",
        "        results.append({\n",
        "            'prompt': prompt,\n",
        "            'result': result\n",
        "        })\n",
        "\n",
        "        # GPU belle\u011fini temizle\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return results\n",
        "\n",
        "# Test fonksiyonu\n",
        "print(\"\ud83e\uddea Toplu i\u015fleme test ediliyor...\")\n",
        "test_prompts = [\n",
        "    \"The weather today is\",\n",
        "    \"Artificial intelligence will\",\n",
        "    \"In the future, humans and robots\"\n",
        "]\n",
        "\n",
        "batch_results = batch_text_processing(test_prompts, model_type=\"gpt2\")\n",
        "print(f\"\u2705 {len(batch_results)} prompt i\u015flendi!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5BjOds6h3Iq"
      },
      "source": [
        "Bu fonksiyon, birden fazla prompt\u2019u GPT-2 veya BERT ile toplu olarak i\u015fler, sonu\u00e7lar\u0131 depolar ve GPU belle\u011fini y\u00f6netir. Test, \u00fc\u00e7 prompt\u2019un GPT-2 ile i\u015flendi\u011fini do\u011frular. \u00d6nceki fonksiyonlara k\u0131yasla, toplu i\u015flem yapabilme ve ilerleme takibi gibi \u00f6zellikleriyle daha \u00f6l\u00e7eklenebilir ve kullan\u0131c\u0131 dostudur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 10,
          "status": "ok",
          "timestamp": 1752825302441,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "9onz7p9Fh3tG",
        "outputId": "aa1cccd3-9f2d-42b4-dc79-f2eaebf52902"
      },
      "outputs": [],
      "source": [
        "#Fonksiyon \u00d6zeti\n",
        "\n",
        "print_separator(\"Fonksiyon \u00d6zeti\")\n",
        "\n",
        "functions_summary = {\n",
        "    'Fonksiyon': [\n",
        "        'generate_text_gpt2',\n",
        "        'analyze_text_bert',\n",
        "        'advanced_text_completion',\n",
        "        'compare_models',\n",
        "        'batch_text_processing'\n",
        "    ],\n",
        "    'A\u00e7\u0131klama': [\n",
        "        'GPT-2 ile metin tamamlama',\n",
        "        'BERT ile metin analizi',\n",
        "        'Geli\u015fmi\u015f metin tamamlama',\n",
        "        'Model kar\u015f\u0131la\u015ft\u0131rmas\u0131',\n",
        "        'Toplu metin i\u015fleme'\n",
        "    ],\n",
        "    'Durum': [\n",
        "        '\u2705 Haz\u0131r',\n",
        "        '\u2705 Haz\u0131r',\n",
        "        '\u2705 Haz\u0131r',\n",
        "        '\u2705 Haz\u0131r',\n",
        "        '\u2705 Haz\u0131r'\n",
        "    ]\n",
        "}\n",
        "\n",
        "df_functions = pd.DataFrame(functions_summary)\n",
        "print(df_functions.to_string(index=False))\n",
        "print(\"\\n\ud83c\udfaf T\u00fcm fonksiyonlar haz\u0131r! Gradio aray\u00fcz\u00fcne ge\u00e7ebilirsiniz.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGVP3OKYh8GA"
      },
      "source": [
        "# **4- GRAD\u0130O ARAY\u00dcZ\u00dc**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 589,
          "status": "ok",
          "timestamp": 1752825333692,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "ors0dkPwh_Hq",
        "outputId": "51a4df41-1c5d-40e0-ca1a-0373781337c2"
      },
      "outputs": [],
      "source": [
        "#Basit Metin Tamamlama Aray\u00fcz\u00fc\n",
        "\n",
        "print_separator(\"Basit Gradio Aray\u00fcz\u00fc\")\n",
        "\n",
        "def simple_text_completion_interface(prompt, max_length, temperature, num_sequences):\n",
        "    \"\"\"\n",
        "    Basit metin tamamlama aray\u00fcz\u00fc i\u00e7in fonksiyon\n",
        "    \"\"\"\n",
        "    if not prompt.strip():\n",
        "        return \"\u274c L\u00fctfen bir metin girin!\"\n",
        "\n",
        "    try:\n",
        "        # Parametreleri kontrol et\n",
        "        max_length = int(max_length)\n",
        "        temperature = float(temperature)\n",
        "        num_sequences = int(num_sequences)\n",
        "\n",
        "        # GPT-2 ile metin \u00fcret\n",
        "        results = generate_text_gpt2(\n",
        "            prompt=prompt,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            num_return_sequences=num_sequences\n",
        "        )\n",
        "\n",
        "        if isinstance(results, list):\n",
        "            # Sonu\u00e7lar\u0131 formatla\n",
        "            output = f\"\ud83c\udfaf **Ba\u015flang\u0131\u00e7:** {prompt}\\n\\n\"\n",
        "            for i, result in enumerate(results):\n",
        "                output += f\"**Sonu\u00e7 {i+1}:**\\n{result['text']}\\n\\n\"\n",
        "                output += f\"**Yeni Eklenen:** {result['new_text']}\\n\\n\"\n",
        "                output += \"---\\n\\n\"\n",
        "            return output\n",
        "        else:\n",
        "            return f\"\u274c Hata: {results}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"\u274c Hata: {str(e)}\"\n",
        "\n",
        "# Basit aray\u00fcz olu\u015ftur\n",
        "simple_interface = gr.Interface(\n",
        "    fn=simple_text_completion_interface,\n",
        "    inputs=[\n",
        "        gr.Textbox(\n",
        "            label=\"Metin Giri\u015fi\",\n",
        "            placeholder=\"Tamamlanmas\u0131n\u0131 istedi\u011finiz metni yaz\u0131n...\",\n",
        "            lines=3\n",
        "        ),\n",
        "        gr.Slider(\n",
        "            minimum=10,\n",
        "            maximum=100,\n",
        "            value=50,\n",
        "            label=\"Maksimum Uzunluk\"\n",
        "        ),\n",
        "        gr.Slider(\n",
        "            minimum=0.1,\n",
        "            maximum=2.0,\n",
        "            value=0.7,\n",
        "            step=0.1,\n",
        "            label=\"Temperature (Yarat\u0131c\u0131l\u0131k)\"\n",
        "        ),\n",
        "        gr.Slider(\n",
        "            minimum=1,\n",
        "            maximum=5,\n",
        "            value=1,\n",
        "            step=1,\n",
        "            label=\"Sonu\u00e7 Say\u0131s\u0131\"\n",
        "        )\n",
        "    ],\n",
        "    outputs=gr.Textbox(\n",
        "        label=\"Sonu\u00e7lar\",\n",
        "        lines=10\n",
        "    ),\n",
        "    title=\"\ud83e\udd16 GPT-2 Metin Tamamlama\",\n",
        "    description=\"GPT-2 modeli ile metin tamamlama yap\u0131n\",\n",
        "    examples=[\n",
        "        [\"The future of artificial intelligence is\", 50, 0.7, 1],\n",
        "        [\"Once upon a time in a distant land\", 60, 0.8, 2],\n",
        "        [\"Technology has revolutionized the way we\", 40, 0.6, 1]\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"\u2705 Basit aray\u00fcz olu\u015fturuldu!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gip7xil4iA3J"
      },
      "source": [
        "Bu kod, GPT-2 modelini kullanarak metin tamamlama i\u00e7in bir Gradio aray\u00fcz\u00fc olu\u015fturur. Kullan\u0131c\u0131, metin giri\u015fi ve parametreleri (uzunluk, yarat\u0131c\u0131l\u0131k, sonu\u00e7 say\u0131s\u0131) ayarlayarak kolayca metin \u00fcretebilir. \u00d6nceki fonksiyonlara g\u00f6re, teknik bilgiye ihtiya\u00e7 duymadan kullan\u0131labilen g\u00f6rsel bir aray\u00fcz sunmas\u0131yla \u00f6ne \u00e7\u0131kar.\n",
        "\n",
        "Parametreler:\n",
        "\n",
        "prompt: Kullan\u0131c\u0131n\u0131n girdi\u011fi ba\u015flang\u0131\u00e7 metni.\n",
        "\n",
        "max_length: \u00dcretilecek metnin maksimum uzunlu\u011fu (token say\u0131s\u0131).\n",
        "\n",
        "temperature: Yarat\u0131c\u0131l\u0131k seviyesi (0.1-2.0 aras\u0131nda).\n",
        "\n",
        "num_sequences: \u00dcretilecek farkl\u0131 metin say\u0131s\u0131.\n",
        "\n",
        "\n",
        "[ ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 167,
          "status": "ok",
          "timestamp": 1752825366877,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "Q3AmHHcoiFCN",
        "outputId": "1d5968b4-8d27-478a-805a-a1ad89b600e0"
      },
      "outputs": [],
      "source": [
        "#Geli\u015fmi\u015f Model Kar\u015f\u0131la\u015ft\u0131rma Aray\u00fcz\u00fc\n",
        "\n",
        "print_separator(\"Geli\u015fmi\u015f Kar\u015f\u0131la\u015ft\u0131rma Aray\u00fcz\u00fc\")\n",
        "\n",
        "def advanced_comparison_interface(prompt, max_length, temperature, analyze_bert):\n",
        "    \"\"\"\n",
        "    GPT-2 ve BERT kar\u015f\u0131la\u015ft\u0131rma aray\u00fcz\u00fc\n",
        "    \"\"\"\n",
        "    if not prompt.strip():\n",
        "        return \"\u274c L\u00fctfen bir metin girin!\", \"\"\n",
        "\n",
        "    try:\n",
        "        # GPT-2 sonucu\n",
        "        gpt2_results = generate_text_gpt2(\n",
        "            prompt=prompt,\n",
        "            max_length=int(max_length),\n",
        "            temperature=float(temperature),\n",
        "            num_return_sequences=1\n",
        "        )\n",
        "\n",
        "        # GPT-2 \u00e7\u0131kt\u0131s\u0131n\u0131 formatla\n",
        "        if isinstance(gpt2_results, list):\n",
        "            gpt2_output = f\"\ud83c\udfaf **Ba\u015flang\u0131\u00e7:** {prompt}\\n\\n\"\n",
        "            gpt2_output += f\"**GPT-2 Sonucu:**\\n{gpt2_results[0]['text']}\\n\\n\"\n",
        "            gpt2_output += f\"**Yeni Eklenen:** {gpt2_results[0]['new_text']}\\n\\n\"\n",
        "            gpt2_output += f\"**Token Say\u0131s\u0131:** {len(gpt2_results[0]['text'].split())}\\n\"\n",
        "        else:\n",
        "            gpt2_output = f\"\u274c GPT-2 Hatas\u0131: {gpt2_results}\"\n",
        "\n",
        "        # BERT analizi\n",
        "        bert_output = \"\"\n",
        "        if analyze_bert:\n",
        "            bert_results = analyze_text_bert(prompt, return_attention=True)\n",
        "\n",
        "            if isinstance(bert_results, dict):\n",
        "                bert_output = f\"\ud83d\udd0d **BERT Analizi:**\\n\\n\"\n",
        "                bert_output += f\"**Token Say\u0131s\u0131:** {bert_results['token_count']}\\n\"\n",
        "                bert_output += f\"**Embedding Boyutu:** {bert_results['embedding_dim']}\\n\\n\"\n",
        "                bert_output += f\"**Tokenlar:**\\n\"\n",
        "                for i, token in enumerate(bert_results['tokens'][:10]):  # \u0130lk 10 token\n",
        "                    bert_output += f\"{i+1}. {token}\\n\"\n",
        "\n",
        "                if len(bert_results['tokens']) > 10:\n",
        "                    bert_output += f\"... ve {len(bert_results['tokens']) - 10} token daha\\n\"\n",
        "            else:\n",
        "                bert_output = f\"\u274c BERT Hatas\u0131: {bert_results}\"\n",
        "\n",
        "        return gpt2_output, bert_output\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"\u274c Hata: {str(e)}\", \"\"\n",
        "\n",
        "# Geli\u015fmi\u015f aray\u00fcz olu\u015ftur\n",
        "advanced_interface = gr.Interface(\n",
        "    fn=advanced_comparison_interface,\n",
        "    inputs=[\n",
        "        gr.Textbox(\n",
        "            label=\"Metin Giri\u015fi\",\n",
        "            placeholder=\"Analiz edilecek metni yaz\u0131n...\",\n",
        "            lines=3\n",
        "        ),\n",
        "        gr.Slider(\n",
        "            minimum=10,\n",
        "            maximum=100,\n",
        "            value=50,\n",
        "            label=\"GPT-2 Maksimum Uzunluk\"\n",
        "        ),\n",
        "        gr.Slider(\n",
        "            minimum=0.1,\n",
        "            maximum=2.0,\n",
        "            value=0.7,\n",
        "            step=0.1,\n",
        "            label=\"GPT-2 Temperature\"\n",
        "        ),\n",
        "        gr.Checkbox(\n",
        "            label=\"BERT Analizi Yap\",\n",
        "            value=True\n",
        "        )\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(\n",
        "            label=\"GPT-2 Sonu\u00e7lar\u0131\",\n",
        "            lines=8\n",
        "        ),\n",
        "        gr.Textbox(\n",
        "            label=\"BERT Analizi\",\n",
        "            lines=8\n",
        "        )\n",
        "    ],\n",
        "    title=\"\ud83d\udd2c Model Kar\u015f\u0131la\u015ft\u0131rma\",\n",
        "    description=\"GPT-2 ve BERT modellerini kar\u015f\u0131la\u015ft\u0131r\u0131n\",\n",
        "    examples=[\n",
        "        [\"The impact of climate change on society\", 50, 0.7, True],\n",
        "        [\"Machine learning algorithms are becoming\", 60, 0.8, True],\n",
        "        [\"In the digital age, privacy has become\", 40, 0.6, False]\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"\u2705 Geli\u015fmi\u015f aray\u00fcz olu\u015fturuldu!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds7AN9fRiI6Y"
      },
      "source": [
        "Ne Yap\u0131ld\u0131?\n",
        "\n",
        "Fonksiyon Tan\u0131m\u0131: advanced_comparison_interface ad\u0131nda bir fonksiyon, verilen bir ba\u015flang\u0131\u00e7 metnini (prompt) GPT-2 ile tamamlar ve iste\u011fe ba\u011fl\u0131 olarak BERT ile analiz eder, sonu\u00e7lar\u0131 formatl\u0131 bir \u015fekilde d\u00f6nd\u00fcr\u00fcr. Parametreler: prompt: Kullan\u0131c\u0131n\u0131n girdi\u011fi metin. max_length: GPT-2 i\u00e7in maksimum token uzunlu\u011fu. temperature: GPT-2 i\u00e7in yarat\u0131c\u0131l\u0131k seviyesi (0.1-2.0). analyze_bert: BERT analizi yap\u0131l\u0131p yap\u0131lmayaca\u011f\u0131n\u0131 belirler (checkbox ile kontrol edilir)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 619,
          "status": "ok",
          "timestamp": 1752825384757,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "FvoOaaQqiLrC",
        "outputId": "b5ac71fa-e6cf-4b8b-f1f4-5b6a7c1c718a"
      },
      "outputs": [],
      "source": [
        "#Toplu \u0130\u015fleme Aray\u00fcz\u00fc\n",
        "\n",
        "print_separator(\"Toplu \u0130\u015fleme Aray\u00fcz\u00fc\")\n",
        "\n",
        "def batch_processing_interface(prompts_text, max_length, temperature):\n",
        "    \"\"\"\n",
        "    Toplu metin i\u015fleme aray\u00fcz\u00fc\n",
        "    \"\"\"\n",
        "    if not prompts_text.strip():\n",
        "        return \"\u274c L\u00fctfen en az bir prompt girin!\"\n",
        "\n",
        "    try:\n",
        "        # Prompts'lar\u0131 ay\u0131r (her sat\u0131r bir prompt)\n",
        "        prompts = [p.strip() for p in prompts_text.split('\\n') if p.strip()]\n",
        "\n",
        "        if len(prompts) == 0:\n",
        "            return \"\u274c Ge\u00e7erli prompt bulunamad\u0131!\"\n",
        "\n",
        "        if len(prompts) > 10:\n",
        "            return \"\u274c Maksimum 10 prompt i\u015flenebilir!\"\n",
        "\n",
        "        # Toplu i\u015fleme\n",
        "        results = []\n",
        "        for i, prompt in enumerate(prompts):\n",
        "            gpt2_result = generate_text_gpt2(\n",
        "                prompt=prompt,\n",
        "                max_length=int(max_length),\n",
        "                temperature=float(temperature),\n",
        "                num_return_sequences=1\n",
        "            )\n",
        "\n",
        "            if isinstance(gpt2_result, list):\n",
        "                results.append({\n",
        "                    'prompt': prompt,\n",
        "                    'result': gpt2_result[0]['text'],\n",
        "                    'new_text': gpt2_result[0]['new_text']\n",
        "                })\n",
        "            else:\n",
        "                results.append({\n",
        "                    'prompt': prompt,\n",
        "                    'result': f\"Hata: {gpt2_result}\",\n",
        "                    'new_text': \"\"\n",
        "                })\n",
        "\n",
        "        # Sonu\u00e7lar\u0131 formatla\n",
        "        output = f\"\ud83d\udcca **Toplu \u0130\u015fleme Sonu\u00e7lar\u0131** ({len(results)} prompt)\\n\\n\"\n",
        "        for i, result in enumerate(results):\n",
        "            output += f\"**{i+1}. Prompt:** {result['prompt']}\\n\"\n",
        "            output += f\"**Sonu\u00e7:** {result['result']}\\n\"\n",
        "            output += f\"**Yeni Eklenen:** {result['new_text']}\\n\"\n",
        "            output += \"---\\n\\n\"\n",
        "\n",
        "        return output\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"\u274c Hata: {str(e)}\"\n",
        "\n",
        "# Toplu i\u015fleme aray\u00fcz\u00fc\n",
        "batch_interface = gr.Interface(\n",
        "    fn=batch_processing_interface,\n",
        "    inputs=[\n",
        "        gr.Textbox(\n",
        "            label=\"Prompts (Her sat\u0131ra bir prompt)\",\n",
        "            placeholder=\"The future of AI is\\nTechnology will help us\\nIn 2030, robots will\",\n",
        "            lines=5\n",
        "        ),\n",
        "        gr.Slider(\n",
        "            minimum=10,\n",
        "            maximum=80,\n",
        "            value=40,\n",
        "            label=\"Maksimum Uzunluk\"\n",
        "        ),\n",
        "        gr.Slider(\n",
        "            minimum=0.1,\n",
        "            maximum=2.0,\n",
        "            value=0.7,\n",
        "            step=0.1,\n",
        "            label=\"Temperature\"\n",
        "        )\n",
        "    ],\n",
        "    outputs=gr.Textbox(\n",
        "        label=\"Toplu \u0130\u015fleme Sonu\u00e7lar\u0131\",\n",
        "        lines=15\n",
        "    ),\n",
        "    title=\"\ud83d\udce6 Toplu Metin \u0130\u015fleme\",\n",
        "    description=\"Birden fazla prompt'u ayn\u0131 anda i\u015fleyin (Max 10 prompt)\",\n",
        "    examples=[\n",
        "        [\"The future of AI is\\nTechnology will help us\\nIn 2030, robots will\", 40, 0.7],\n",
        "        [\"Climate change affects\\nRenewable energy sources\\nSustainable development means\", 50, 0.8]\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"\u2705 Toplu i\u015fleme aray\u00fcz\u00fc olu\u015fturuldu!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRgQ1UbViOYA"
      },
      "source": [
        "**Ne Yap\u0131ld\u0131?**\n",
        "Fonksiyon Tan\u0131m\u0131: batch_processing_interface ad\u0131nda bir fonksiyon, kullan\u0131c\u0131dan al\u0131nan birden fazla prompt\u2019u GPT-2 ile toplu olarak i\u015fler ve sonu\u00e7lar\u0131 formatl\u0131 bir \u015fekilde d\u00f6nd\u00fcr\u00fcr. Parametreler: prompts_text: Kullan\u0131c\u0131n\u0131n her sat\u0131ra bir prompt yazd\u0131\u011f\u0131 metin alan\u0131 (sat\u0131rlarla ayr\u0131lm\u0131\u015f). max_length: Her prompt i\u00e7in \u00fcretilecek metnin maksimum uzunlu\u011fu (token say\u0131s\u0131). temperature: GPT-2 i\u00e7in yarat\u0131c\u0131l\u0131k seviyesi (0.1-2.0).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 383,
          "status": "ok",
          "timestamp": 1752825403823,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "yz40k7jsiQQk",
        "outputId": "33a1d36a-6541-41a1-cadc-0b9ced834b77"
      },
      "outputs": [],
      "source": [
        "#Sekmeli Ana Aray\u00fcz\n",
        "\n",
        "print_separator(\"Ana Sekmeli Aray\u00fcz\")\n",
        "\n",
        "# T\u00fcm aray\u00fczleri sekmeler halinde birle\u015ftir\n",
        "main_interface = gr.TabbedInterface(\n",
        "    [simple_interface, advanced_interface, batch_interface],\n",
        "    tab_names=[\"\ud83e\udd16 Basit Tamamlama\", \"\ud83d\udd2c Model Kar\u015f\u0131la\u015ft\u0131rma\", \"\ud83d\udce6 Toplu \u0130\u015fleme\"]\n",
        ")\n",
        "\n",
        "print(\"\u2705 Ana sekmeli aray\u00fcz olu\u015fturuldu!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 160,
          "status": "ok",
          "timestamp": 1752825409955,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "-ttcQNrxiR4U",
        "outputId": "7e67e515-21d4-49cf-d7d6-6abc88e0c8f8"
      },
      "outputs": [],
      "source": [
        "#\u00d6zel Stil ve Tema\n",
        "\n",
        "print_separator(\"\u00d6zel Tema ve Stil\")\n",
        "\n",
        "# \u00d6zel CSS\n",
        "custom_css = \"\"\"\n",
        ".gradio-container {\n",
        "    font-family: 'Arial', sans-serif;\n",
        "    max-width: 1200px;\n",
        "    margin: 0 auto;\n",
        "}\n",
        "\n",
        ".gr-button {\n",
        "    background: linear-gradient(45deg, #667eea 0%, #764ba2 100%);\n",
        "    border: none;\n",
        "    color: white;\n",
        "    font-weight: bold;\n",
        "}\n",
        "\n",
        ".gr-button:hover {\n",
        "    transform: translateY(-2px);\n",
        "    box-shadow: 0 5px 15px rgba(0,0,0,0.2);\n",
        "}\n",
        "\n",
        ".gr-textbox {\n",
        "    border-radius: 10px;\n",
        "    border: 2px solid #e1e5e9;\n",
        "}\n",
        "\n",
        ".gr-panel {\n",
        "    border-radius: 15px;\n",
        "    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n",
        "}\n",
        "\n",
        ".gr-form {\n",
        "    background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);\n",
        "    padding: 20px;\n",
        "    border-radius: 15px;\n",
        "}\n",
        "\n",
        "h1 {\n",
        "    color: #2c3e50;\n",
        "    text-align: center;\n",
        "    font-size: 2.5em;\n",
        "    margin-bottom: 10px;\n",
        "}\n",
        "\n",
        ".description {\n",
        "    text-align: center;\n",
        "    color: #7f8c8d;\n",
        "    font-size: 1.1em;\n",
        "    margin-bottom: 20px;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Tema ile birlikte aray\u00fcz\n",
        "themed_interface = gr.TabbedInterface(\n",
        "    [simple_interface, advanced_interface, batch_interface],\n",
        "    tab_names=[\"\ud83e\udd16 Basit Tamamlama\", \"\ud83d\udd2c Model Kar\u015f\u0131la\u015ft\u0131rma\", \"\ud83d\udce6 Toplu \u0130\u015fleme\"],\n",
        "    css=custom_css,\n",
        "    theme=gr.themes.Soft()\n",
        ")\n",
        "\n",
        "print(\"\u2705 \u00d6zel tema uyguland\u0131!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 27,
          "status": "ok",
          "timestamp": 1752825417051,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "NqRKAQP_iTk3",
        "outputId": "ecca32f1-2179-4381-e36c-8c4ea04dcf33"
      },
      "outputs": [],
      "source": [
        "#Aray\u00fcz\u00fc Ba\u015flatma\n",
        "\n",
        "print_separator(\"Aray\u00fcz\u00fc Ba\u015flatma\")\n",
        "\n",
        "def launch_interface(interface_type=\"main\"):\n",
        "    \"\"\"\n",
        "    Se\u00e7ilen aray\u00fcz\u00fc ba\u015flat\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if interface_type == \"simple\":\n",
        "            print(\"\ud83d\ude80 Basit aray\u00fcz ba\u015flat\u0131l\u0131yor...\")\n",
        "            simple_interface.launch(\n",
        "                share=True,\n",
        "                debug=True,\n",
        "                server_port=7860,\n",
        "                inbrowser=True\n",
        "            )\n",
        "        elif interface_type == \"advanced\":\n",
        "            print(\"\ud83d\ude80 Geli\u015fmi\u015f aray\u00fcz ba\u015flat\u0131l\u0131yor...\")\n",
        "            advanced_interface.launch(\n",
        "                share=True,\n",
        "                debug=True,\n",
        "                server_port=7861,\n",
        "                inbrowser=True\n",
        "            )\n",
        "        elif interface_type == \"batch\":\n",
        "            print(\"\ud83d\ude80 Toplu i\u015fleme aray\u00fcz\u00fc ba\u015flat\u0131l\u0131yor...\")\n",
        "            batch_interface.launch(\n",
        "                share=True,\n",
        "                debug=True,\n",
        "                server_port=7862,\n",
        "                inbrowser=True\n",
        "            )\n",
        "        else:\n",
        "            print(\"\ud83d\ude80 Ana aray\u00fcz ba\u015flat\u0131l\u0131yor...\")\n",
        "            themed_interface.launch(\n",
        "                share=True,\n",
        "                debug=True,\n",
        "                server_port=7860,\n",
        "                inbrowser=True\n",
        "            )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Aray\u00fcz ba\u015flatma hatas\u0131: {e}\")\n",
        "\n",
        "# Aray\u00fcz durumu kontrol\n",
        "print(\"\ud83d\udcca Aray\u00fcz Durumu:\")\n",
        "print(\"\u2705 Basit Tamamlama: Haz\u0131r\")\n",
        "print(\"\u2705 Model Kar\u015f\u0131la\u015ft\u0131rma: Haz\u0131r\")\n",
        "print(\"\u2705 Toplu \u0130\u015fleme: Haz\u0131r\")\n",
        "print(\"\u2705 Ana Sekmeli Aray\u00fcz: Haz\u0131r\")\n",
        "print(\"\u2705 \u00d6zel Tema: Uyguland\u0131\")\n",
        "\n",
        "print(\"\\n\ud83c\udfaf Aray\u00fcz\u00fc ba\u015flatmak i\u00e7in:\")\n",
        "print(\"launch_interface('main')     # Ana aray\u00fcz (\u00f6nerilen)\")\n",
        "print(\"launch_interface('simple')   # Basit aray\u00fcz\")\n",
        "print(\"launch_interface('advanced') # Geli\u015fmi\u015f aray\u00fcz\")\n",
        "print(\"launch_interface('batch')    # Toplu i\u015fleme\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "executionInfo": {
          "elapsed": 16101,
          "status": "ok",
          "timestamp": 1752825991683,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "vWENaY3JiWlA",
        "outputId": "ff688bb1-3d9e-4130-8451-3fd0d22bd60e"
      },
      "outputs": [],
      "source": [
        "#H\u0131zl\u0131 Ba\u015flatma\n",
        "\n",
        "print_separator(\"H\u0131zl\u0131 Ba\u015flatma\")\n",
        "\n",
        "# Ana aray\u00fcz\u00fc otomatik ba\u015flat\n",
        "print(\"\ud83d\ude80 Ana aray\u00fcz otomatik ba\u015flat\u0131l\u0131yor...\")\n",
        "launch_interface(\"main\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIu9YeV8iZX8"
      },
      "source": [
        "# **5- MODEL KAR\u015eILA\u015eTIRMASI**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 59,
          "status": "ok",
          "timestamp": 1752826859281,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "XXISaVvhi8wf",
        "outputId": "ffe2dd23-f66b-4ae8-ad24-f1455e8f50b1"
      },
      "outputs": [],
      "source": [
        "print_separator(\"Detayl\u0131 Model Kar\u015f\u0131la\u015ft\u0131rma\")\n",
        "\n",
        "def detailed_model_comparison(prompt, max_length=50, temperature=0.7, num_sequences=3):\n",
        "    \"\"\"\n",
        "    GPT-2 ve BERT modellerini detayl\u0131 kar\u015f\u0131la\u015ft\u0131r\u0131r\n",
        "    \"\"\"\n",
        "    comparison_results = {\n",
        "        'prompt': prompt,\n",
        "        'gpt2_results': [],\n",
        "        'bert_results': {},\n",
        "        'performance_metrics': {},\n",
        "        'analysis': {}\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # GPT-2 ile metin \u00fcretimi\n",
        "        print(\"\ud83d\udd04 GPT-2 ile metin \u00fcretiliyor...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        gpt2_outputs = generate_text_gpt2(\n",
        "            prompt=prompt,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            num_return_sequences=num_sequences\n",
        "        )\n",
        "\n",
        "        gpt2_time = time.time() - start_time\n",
        "\n",
        "        if isinstance(gpt2_outputs, list):\n",
        "            comparison_results['gpt2_results'] = gpt2_outputs\n",
        "            comparison_results['performance_metrics']['gpt2_time'] = gpt2_time\n",
        "            comparison_results['performance_metrics']['gpt2_sequences'] = len(gpt2_outputs)\n",
        "\n",
        "        # BERT ile analiz\n",
        "        print(\"\ud83d\udd04 BERT ile analiz yap\u0131l\u0131yor...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        bert_output = analyze_text_bert(prompt, return_attention=True)\n",
        "\n",
        "        bert_time = time.time() - start_time\n",
        "\n",
        "        if isinstance(bert_output, dict):\n",
        "            comparison_results['bert_results'] = bert_output\n",
        "            comparison_results['performance_metrics']['bert_time'] = bert_time\n",
        "\n",
        "        # Performans analizi\n",
        "        comparison_results['analysis'] = analyze_model_performance(\n",
        "            gpt2_outputs, bert_output, gpt2_time, bert_time\n",
        "        )\n",
        "\n",
        "        return comparison_results\n",
        "\n",
        "    except Exception as e:\n",
        "        comparison_results['error'] = str(e)\n",
        "        return comparison_results\n",
        "\n",
        "def analyze_model_performance(gpt2_results, bert_results, gpt2_time, bert_time):\n",
        "    \"\"\"\n",
        "    Model performans\u0131n\u0131 analiz eder\n",
        "    \"\"\"\n",
        "    analysis = {}\n",
        "\n",
        "    try:\n",
        "        # GPT-2 analizi\n",
        "        if isinstance(gpt2_results, list) and len(gpt2_results) > 0:\n",
        "            generated_texts = [result['new_text'] for result in gpt2_results]\n",
        "\n",
        "            analysis['gpt2'] = {\n",
        "                'avg_length': np.mean([len(text.split()) for text in generated_texts]),\n",
        "                'max_length': max([len(text.split()) for text in generated_texts]),\n",
        "                'min_length': min([len(text.split()) for text in generated_texts]),\n",
        "                'generation_time': gpt2_time,\n",
        "                'words_per_second': sum([len(text.split()) for text in generated_texts]) / gpt2_time if gpt2_time > 0 else 0\n",
        "            }\n",
        "\n",
        "        # BERT analizi\n",
        "        if isinstance(bert_results, dict):\n",
        "            analysis['bert'] = {\n",
        "                'token_count': bert_results.get('token_count', 0),\n",
        "                'embedding_dim': bert_results.get('embedding_dim', 0),\n",
        "                'processing_time': bert_time,\n",
        "                'tokens_per_second': bert_results.get('token_count', 0) / bert_time if bert_time > 0 else 0\n",
        "            }\n",
        "\n",
        "        # Kar\u015f\u0131la\u015ft\u0131rma\n",
        "        if 'gpt2' in analysis and 'bert' in analysis:\n",
        "            analysis['comparison'] = {\n",
        "                'speed_ratio': bert_time / gpt2_time if gpt2_time > 0 else 0,\n",
        "                'bert_faster': bert_time < gpt2_time,\n",
        "                'gpt2_creativity': analysis['gpt2']['avg_length'] > 10,\n",
        "                'bert_understanding': analysis['bert']['token_count'] > 5\n",
        "            }\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    except Exception as e:\n",
        "        return {'error': str(e)}\n",
        "\n",
        "# Test fonksiyonu\n",
        "print(\"\ud83e\uddea Detayl\u0131 kar\u015f\u0131la\u015ft\u0131rma test ediliyor...\")\n",
        "import time\n",
        "\n",
        "test_comparison = detailed_model_comparison(\n",
        "    \"The future of artificial intelligence\",\n",
        "    max_length=40,\n",
        "    temperature=0.8,\n",
        "    num_sequences=2\n",
        ")\n",
        "\n",
        "if 'error' not in test_comparison:\n",
        "    print(\"\u2705 Detayl\u0131 kar\u015f\u0131la\u015ft\u0131rma testi ba\u015far\u0131l\u0131!\")\n",
        "else:\n",
        "    print(f\"\u274c Test hatas\u0131: {test_comparison['error']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyRKsRkUm0VZ"
      },
      "source": [
        "Kod, GPT-2 ve BERT modellerini kar\u015f\u0131la\u015ft\u0131r\u0131yor:\n",
        "\n",
        "GPT-2: Verilen prompt ile metin \u00fcretir, \u00fcretim h\u0131z\u0131 ve metin uzunluklar\u0131 analiz edilir.\n",
        "BERT: Promptu analiz eder, token say\u0131s\u0131 ve i\u015fleme h\u0131z\u0131 hesaplan\u0131r.\n",
        "Kar\u015f\u0131la\u015ft\u0131rma: H\u0131z, yarat\u0131c\u0131l\u0131k (GPT-2) ve anlama (BERT) a\u00e7\u0131s\u0131ndan modeller de\u011ferlendirilir.\n",
        "Test: \"The future of artificial intelligence\" promptu ile test edilir; ba\u015far\u0131l\u0131ysa onay mesaj\u0131, de\u011filse hata g\u00f6sterilir. Sonu\u00e7lar, performans metrikleriyle birlikte bir s\u00f6zl\u00fckte toplan\u0131r."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 17,
          "status": "ok",
          "timestamp": 1752826013114,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "SjaugdK4kl9q",
        "outputId": "9779b36d-e084-49d9-a653-f77b9487a0c5"
      },
      "outputs": [],
      "source": [
        "print_separator(\"Performans Metrikleri\")\n",
        "\n",
        "def calculate_performance_metrics(comparison_results):\n",
        "    \"\"\"\n",
        "    Performans metriklerini hesaplar ve g\u00f6rselle\u015ftirir\n",
        "    \"\"\"\n",
        "    metrics = {}\n",
        "\n",
        "    try:\n",
        "        # GPT-2 metrikleri\n",
        "        if 'gpt2_results' in comparison_results and comparison_results['gpt2_results']:\n",
        "            gpt2_texts = [result['new_text'] for result in comparison_results['gpt2_results']]\n",
        "\n",
        "            metrics['gpt2'] = {\n",
        "                'total_words': sum([len(text.split()) for text in gpt2_texts]),\n",
        "                'avg_word_length': np.mean([np.mean([len(word) for word in text.split()]) for text in gpt2_texts if text.split()]),\n",
        "                'unique_words': len(set(' '.join(gpt2_texts).split())),\n",
        "                'repetition_rate': calculate_repetition_rate(gpt2_texts),\n",
        "                'creativity_score': calculate_creativity_score(gpt2_texts)\n",
        "            }\n",
        "\n",
        "        # BERT metrikleri\n",
        "        if 'bert_results' in comparison_results and comparison_results['bert_results']:\n",
        "            bert_data = comparison_results['bert_results']\n",
        "\n",
        "            metrics['bert'] = {\n",
        "                'vocabulary_coverage': bert_data.get('token_count', 0),\n",
        "                'embedding_density': bert_data.get('embedding_dim', 0),\n",
        "                'token_diversity': calculate_token_diversity(bert_data.get('tokens', [])),\n",
        "                'processing_efficiency': bert_data.get('token_count', 0) / comparison_results['performance_metrics'].get('bert_time', 1)\n",
        "            }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        return {'error': str(e)}\n",
        "\n",
        "def calculate_repetition_rate(texts):\n",
        "    \"\"\"\n",
        "    Tekrar oran\u0131n\u0131 hesaplar\n",
        "    \"\"\"\n",
        "    if not texts:\n",
        "        return 0\n",
        "\n",
        "    all_words = ' '.join(texts).split()\n",
        "    unique_words = set(all_words)\n",
        "\n",
        "    return 1 - (len(unique_words) / len(all_words)) if all_words else 0\n",
        "\n",
        "def calculate_creativity_score(texts):\n",
        "    \"\"\"\n",
        "    Yarat\u0131c\u0131l\u0131k skorunu hesaplar\n",
        "    \"\"\"\n",
        "    if not texts:\n",
        "        return 0\n",
        "\n",
        "    # Basit yarat\u0131c\u0131l\u0131k skoru: kelime \u00e7e\u015fitlili\u011fi ve c\u00fcmle uzunlu\u011fu\n",
        "    all_words = ' '.join(texts).split()\n",
        "    unique_words = set(all_words)\n",
        "    avg_sentence_length = np.mean([len(text.split()) for text in texts])\n",
        "\n",
        "    diversity_score = len(unique_words) / len(all_words) if all_words else 0\n",
        "    length_score = min(avg_sentence_length / 20, 1)  # Normalize to 0-1\n",
        "\n",
        "    return (diversity_score + length_score) / 2\n",
        "\n",
        "def calculate_token_diversity(tokens):\n",
        "    \"\"\"\n",
        "    Token \u00e7e\u015fitlili\u011fini hesaplar\n",
        "    \"\"\"\n",
        "    if not tokens:\n",
        "        return 0\n",
        "\n",
        "    # \u00d6zel tokenlar\u0131 filtrele\n",
        "    filtered_tokens = [token for token in tokens if not token.startswith('[') and not token.startswith('#')]\n",
        "\n",
        "    return len(set(filtered_tokens)) / len(filtered_tokens) if filtered_tokens else 0\n",
        "\n",
        "# Test fonksiyonu\n",
        "print(\"\ud83e\uddea Performans metrikleri test ediliyor...\")\n",
        "if 'error' not in test_comparison:\n",
        "    test_metrics = calculate_performance_metrics(test_comparison)\n",
        "    if 'error' not in test_metrics:\n",
        "        print(\"\u2705 Performans metrikleri hesapland\u0131!\")\n",
        "    else:\n",
        "        print(f\"\u274c Metrik hatas\u0131: {test_metrics['error']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 13023,
          "status": "ok",
          "timestamp": 1752830030971,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "PlzB5qbqzjRW",
        "outputId": "5635af3d-bf5f-494d-ab4e-880b79aaa43c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def create_gpt2_bert_style_heatmaps(text=\"The future of artificial intelligence is bright\"):\n",
        "    \"\"\"\n",
        "    GPT-2 i\u00e7in BERT stili attention heatmaps olu\u015fturur\n",
        "    \"\"\"\n",
        "    print(\"\ud83d\udd04 GPT-2 modeli y\u00fckleniyor...\")\n",
        "\n",
        "    # Model ve tokenizer y\u00fckle\n",
        "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=20, truncation=True)\n",
        "\n",
        "    # Attention'lar\u0131 \u00e7\u0131kar\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_attentions=True)\n",
        "\n",
        "    attentions = outputs.attentions\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "    # Token'lar\u0131 temizle\n",
        "    clean_tokens = []\n",
        "    for token in tokens:\n",
        "        if token.startswith('\u0120'):\n",
        "            clean_tokens.append(token[1:])  # \u0120 karakterini kald\u0131r\n",
        "        else:\n",
        "            clean_tokens.append(token)\n",
        "\n",
        "    # BERT stili 6 subplot (3x2) olu\u015ftur\n",
        "    fig, axes = plt.subplots(3, 2, figsize=(16, 20))\n",
        "    fig.suptitle('GPT-2 Attention A\u011f\u0131rl\u0131klar\u0131 - \u00c7oklu Katmanlar ve Kafalar', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Katman ve kafa kombinasyonlar\u0131 (BERT \u00f6rne\u011findeki gibi)\n",
        "    layer_head_combinations = [\n",
        "        (0, 0),   # Katman 0, Kafa 0\n",
        "        (0, 5),   # Katman 0, Kafa 5\n",
        "        (5, 0),   # Katman 5, Kafa 0\n",
        "        (5, 5),   # Katman 5, Kafa 5\n",
        "        (11, 0),  # Katman 11, Kafa 0\n",
        "        (11, 5)   # Katman 11, Kafa 5\n",
        "    ]\n",
        "\n",
        "    for idx, (layer, head) in enumerate(layer_head_combinations):\n",
        "        row = idx // 2\n",
        "        col = idx % 2\n",
        "        ax = axes[row, col]\n",
        "\n",
        "        # Attention matrisini al\n",
        "        attention_matrix = attentions[layer][0, head].numpy()\n",
        "\n",
        "        # Boyut kontrol\u00fc\n",
        "        seq_len = len(clean_tokens)\n",
        "        attention_matrix = attention_matrix[:seq_len, :seq_len]\n",
        "\n",
        "        # GPT-2 causal mask uygula (sadece \u00f6nceki tokenlar)\n",
        "        mask = np.tril(np.ones_like(attention_matrix))\n",
        "        attention_matrix = attention_matrix * mask\n",
        "\n",
        "        # Heatmap \u00e7iz\n",
        "        sns.heatmap(attention_matrix,\n",
        "                   xticklabels=clean_tokens,\n",
        "                   yticklabels=clean_tokens,\n",
        "                   cmap='viridis',\n",
        "                   ax=ax,\n",
        "                   cbar_kws={'label': 'A\u011f\u0131rl\u0131k'},\n",
        "                   vmin=0,\n",
        "                   vmax=1.0)\n",
        "\n",
        "        ax.set_title(f'Katman {layer}, Kafa {head}', fontsize=12, fontweight='bold')\n",
        "        ax.set_xlabel('Tokenlar (X)', fontsize=10)\n",
        "        ax.set_ylabel('Tokenlar (Y)', fontsize=10)\n",
        "\n",
        "        # Etiketleri d\u00f6nd\u00fcr\n",
        "        ax.set_xticklabels(clean_tokens, rotation=45, ha='right')\n",
        "        ax.set_yticklabels(clean_tokens, rotation=0)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return attentions, clean_tokens\n",
        "\n",
        "def create_single_layer_multi_head_heatmaps(text=\"The future of artificial intelligence is bright\", layer_idx=11):\n",
        "    \"\"\"\n",
        "    Tek katman, \u00e7oklu kafa heatmaps (BERT \u00f6rne\u011findeki gibi)\n",
        "    \"\"\"\n",
        "    print(f\"\ud83d\udd04 GPT-2 Katman {layer_idx} - \u00c7oklu Kafa Analizi...\")\n",
        "\n",
        "    # Model ve tokenizer y\u00fckle\n",
        "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=20, truncation=True)\n",
        "\n",
        "    # Attention'lar\u0131 \u00e7\u0131kar\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_attentions=True)\n",
        "\n",
        "    attentions = outputs.attentions\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "    # Token'lar\u0131 temizle\n",
        "    clean_tokens = []\n",
        "    for token in tokens:\n",
        "        if token.startswith('\u0120'):\n",
        "            clean_tokens.append(token[1:])\n",
        "        else:\n",
        "            clean_tokens.append(token)\n",
        "\n",
        "    # 12 kafa i\u00e7in 3x4 subplot\n",
        "    fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
        "    fig.suptitle(f'GPT-2 Katman {layer_idx} - T\u00fcm Attention Kafalar\u0131', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # 12 kafa i\u00e7in\n",
        "    for head_idx in range(12):\n",
        "        row = head_idx // 4\n",
        "        col = head_idx % 4\n",
        "        ax = axes[row, col]\n",
        "\n",
        "        # Attention matrisini al\n",
        "        attention_matrix = attentions[layer_idx][0, head_idx].numpy()\n",
        "\n",
        "        # Boyut kontrol\u00fc\n",
        "        seq_len = len(clean_tokens)\n",
        "        attention_matrix = attention_matrix[:seq_len, :seq_len]\n",
        "\n",
        "        # GPT-2 causal mask uygula\n",
        "        mask = np.tril(np.ones_like(attention_matrix))\n",
        "        attention_matrix = attention_matrix * mask\n",
        "\n",
        "        # Heatmap \u00e7iz\n",
        "        sns.heatmap(attention_matrix,\n",
        "                   xticklabels=clean_tokens,\n",
        "                   yticklabels=clean_tokens,\n",
        "                   cmap='viridis',\n",
        "                   ax=ax,\n",
        "                   cbar_kws={'label': 'A\u011f\u0131rl\u0131k'},\n",
        "                   vmin=0,\n",
        "                   vmax=1.0)\n",
        "\n",
        "        ax.set_title(f'Kafa {head_idx}', fontsize=10, fontweight='bold')\n",
        "        ax.set_xlabel('Tokenlar (X)', fontsize=8)\n",
        "        ax.set_ylabel('Tokenlar (Y)', fontsize=8)\n",
        "\n",
        "        # Etiketleri k\u00fc\u00e7\u00fclt\n",
        "        ax.set_xticklabels(clean_tokens, rotation=45, ha='right', fontsize=8)\n",
        "        ax.set_yticklabels(clean_tokens, rotation=0, fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def create_layer_comparison_heatmaps(text=\"The future of artificial intelligence is bright\"):\n",
        "    \"\"\"\n",
        "    Katman kar\u015f\u0131la\u015ft\u0131rma heatmaps (BERT stili)\n",
        "    \"\"\"\n",
        "    print(\"\ud83d\udd04 GPT-2 Katman Kar\u015f\u0131la\u015ft\u0131rma Analizi...\")\n",
        "\n",
        "    # Model ve tokenizer y\u00fckle\n",
        "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=20, truncation=True)\n",
        "\n",
        "    # Attention'lar\u0131 \u00e7\u0131kar\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_attentions=True)\n",
        "\n",
        "    attentions = outputs.attentions\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "    # Token'lar\u0131 temizle\n",
        "    clean_tokens = []\n",
        "    for token in tokens:\n",
        "        if token.startswith('\u0120'):\n",
        "            clean_tokens.append(token[1:])\n",
        "        else:\n",
        "            clean_tokens.append(token)\n",
        "\n",
        "    # 6 katman i\u00e7in 2x3 subplot\n",
        "    layers_to_show = [0, 2, 5, 8, 10, 11]\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    fig.suptitle('GPT-2 Katman Kar\u015f\u0131la\u015ft\u0131rmas\u0131 - Attention Evrilimi', fontsize=16, fontweight='bold')\n",
        "\n",
        "    for idx, layer_idx in enumerate(layers_to_show):\n",
        "        row = idx // 3\n",
        "        col = idx % 3\n",
        "        ax = axes[row, col]\n",
        "\n",
        "        # \u0130lk kafay\u0131 al (head 0)\n",
        "        attention_matrix = attentions[layer_idx][0, 0].numpy()\n",
        "\n",
        "        # Boyut kontrol\u00fc\n",
        "        seq_len = len(clean_tokens)\n",
        "        attention_matrix = attention_matrix[:seq_len, :seq_len]\n",
        "\n",
        "        # GPT-2 causal mask uygula\n",
        "        mask = np.tril(np.ones_like(attention_matrix))\n",
        "        attention_matrix = attention_matrix * mask\n",
        "\n",
        "        # Heatmap \u00e7iz\n",
        "        sns.heatmap(attention_matrix,\n",
        "                   xticklabels=clean_tokens,\n",
        "                   yticklabels=clean_tokens,\n",
        "                   cmap='viridis',\n",
        "                   ax=ax,\n",
        "                   cbar_kws={'label': 'A\u011f\u0131rl\u0131k'},\n",
        "                   vmin=0,\n",
        "                   vmax=1.0)\n",
        "\n",
        "        ax.set_title(f'Katman {layer_idx}', fontsize=12, fontweight='bold')\n",
        "        ax.set_xlabel('Tokenlar (X)', fontsize=10)\n",
        "        ax.set_ylabel('Tokenlar (Y)', fontsize=10)\n",
        "\n",
        "        # Etiketleri d\u00f6nd\u00fcr\n",
        "        ax.set_xticklabels(clean_tokens, rotation=45, ha='right', fontsize=9)\n",
        "        ax.set_yticklabels(clean_tokens, rotation=0, fontsize=9)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_gpt2_attention_statistics(text=\"The future of artificial intelligence is bright\"):\n",
        "    \"\"\"\n",
        "    GPT-2 attention istatistikleri (BERT \u00f6rne\u011findeki gibi)\n",
        "    \"\"\"\n",
        "    print(\"\ud83d\udcca GPT-2 Attention \u0130statistikleri:\")\n",
        "\n",
        "    # Model ve tokenizer y\u00fckle\n",
        "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=20, truncation=True)\n",
        "\n",
        "    # Attention'lar\u0131 \u00e7\u0131kar\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_attentions=True)\n",
        "\n",
        "    attentions = outputs.attentions\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "    # Token'lar\u0131 temizle\n",
        "    clean_tokens = []\n",
        "    for token in tokens:\n",
        "        if token.startswith('\u0120'):\n",
        "            clean_tokens.append(token[1:])\n",
        "        else:\n",
        "            clean_tokens.append(token)\n",
        "\n",
        "    print(f\"\\n\ud83d\udd0d Analiz Edilen Metin: '{text}'\")\n",
        "    print(f\"\ud83d\udd0d Token Say\u0131s\u0131: {len(clean_tokens)}\")\n",
        "    print(f\"\ud83d\udd0d Tokenlar: {clean_tokens}\")\n",
        "    print(f\"\ud83d\udd0d Katman Say\u0131s\u0131: {len(attentions)}\")\n",
        "    print(f\"\ud83d\udd0d Kafa Say\u0131s\u0131: {attentions[0].shape[1]}\")\n",
        "\n",
        "    # Her katman i\u00e7in istatistikler\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"KATMAN BAZLI \u0130STAT\u0130ST\u0130KLER\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    for layer_idx in range(len(attentions)):\n",
        "        layer_attention = attentions[layer_idx][0]  # \u0130lk batch\n",
        "\n",
        "        # Ortalama ve maksimum attention\n",
        "        avg_attention = layer_attention.mean().item()\n",
        "        max_attention = layer_attention.max().item()\n",
        "\n",
        "        # Diagonal attention (self-attention)\n",
        "        diagonal_vals = []\n",
        "        for head_idx in range(layer_attention.shape[0]):\n",
        "            diag = torch.diagonal(layer_attention[head_idx]).mean().item()\n",
        "            diagonal_vals.append(diag)\n",
        "\n",
        "        avg_diagonal = np.mean(diagonal_vals)\n",
        "\n",
        "        print(f\"Katman {layer_idx:2d}: Ort={avg_attention:.4f}, Max={max_attention:.4f}, Self-Att={avg_diagonal:.4f}\")\n",
        "\n",
        "    return attentions, clean_tokens\n",
        "\n",
        "# Ana \u00e7al\u0131\u015ft\u0131rma fonksiyonu\n",
        "def run_gpt2_bert_style_analysis():\n",
        "    \"\"\"\n",
        "    T\u00fcm GPT-2 BERT stili analizleri \u00e7al\u0131\u015ft\u0131r\n",
        "    \"\"\"\n",
        "    print(\"\ud83d\ude80 GPT-2 BERT Stili Attention Analizi Ba\u015flat\u0131l\u0131yor...\")\n",
        "\n",
        "    test_text = \"The future of artificial intelligence is bright\"\n",
        "\n",
        "    # 1. \u00c7oklu katman-kafa heatmaps\n",
        "    print(\"\\n1\ufe0f\u20e3 \u00c7oklu Katman-Kafa Heatmaps...\")\n",
        "    attentions, tokens = create_gpt2_bert_style_heatmaps(test_text)\n",
        "\n",
        "    # 2. Tek katman, \u00e7oklu kafa heatmaps\n",
        "    print(\"\\n2\ufe0f\u20e3 Tek Katman, \u00c7oklu Kafa Heatmaps...\")\n",
        "    create_single_layer_multi_head_heatmaps(test_text, layer_idx=11)\n",
        "\n",
        "    # 3. Katman kar\u015f\u0131la\u015ft\u0131rma heatmaps\n",
        "    print(\"\\n3\ufe0f\u20e3 Katman Kar\u015f\u0131la\u015ft\u0131rma Heatmaps...\")\n",
        "    create_layer_comparison_heatmaps(test_text)\n",
        "\n",
        "    # 4. \u0130statistiksel analiz\n",
        "    print(\"\\n4\ufe0f\u20e3 \u0130statistiksel Analiz...\")\n",
        "    analyze_gpt2_attention_statistics(test_text)\n",
        "\n",
        "    print(\"\\n\u2705 GPT-2 BERT Stili Attention Analizi Tamamland\u0131!\")\n",
        "\n",
        "# Test \u00e7al\u0131\u015ft\u0131r\n",
        "if __name__ == \"__main__\":\n",
        "    run_gpt2_bert_style_analysis()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a2xtDYP0xPP"
      },
      "source": [
        " Katman 0, Kafa 0:\n",
        "Her kelime kendiyle ilgileniyor (y\u00fcksek dikkat).\n",
        "\n",
        "Di\u011fer kelimelere neredeyse hi\u00e7 bakm\u0131yor.\n",
        "\n",
        "Dikkat da\u011f\u0131l\u0131m\u0131 d\u00fcz bir \u00e7izgi gibi (diagonal).\n",
        "\n",
        "\ud83d\udd39 Katman 0, Kafa 5:\n",
        "Dikkat biraz daha da\u011f\u0131lm\u0131\u015f.\n",
        "\n",
        "\u00d6rne\u011fin \"The\", \"future\" ve \"of\" kelimeleri aras\u0131nda orta d\u00fczey ba\u011flant\u0131lar var.\n",
        "\n",
        "Yine de \u00e7o\u011funlukla kendi kelimesine odakl\u0131.\n",
        "\n",
        "\ud83d\udd39 Katman 5, Kafa 0:\n",
        "Yine \u00e7o\u011funlukla kendine bakan dikkat var.\n",
        "\n",
        "Di\u011fer kelimelere ilgi \u00e7ok d\u00fc\u015f\u00fck.\n",
        "\n",
        "Basit ve tek y\u00f6nl\u00fc dikkat yap\u0131s\u0131.\n",
        "\n",
        "\ud83d\udd39 Katman 5, Kafa 5:\n",
        "En kar\u0131\u015f\u0131k ve zengin yap\u0131 burada.\n",
        "\n",
        "\u00d6rne\u011fin \"The\" kelimesi, \"bright\" kelimesine g\u00fc\u00e7l\u00fc \u015fekilde dikkat ediyor.\n",
        "\n",
        "Hem kendi kelimesine bak\u0131yor hem de di\u011fer \u00f6nemli kelimelere.\n",
        "\n",
        "\ud83d\udd39 Katman 11, Kafa 0:\n",
        "H\u00e2l\u00e2 b\u00fcy\u00fck \u00f6l\u00e7\u00fcde kendi kelimesine odakl\u0131.\n",
        "\n",
        "Biraz ba\u015fka kelimelere ilgi var ama zay\u0131f.\n",
        "\n",
        "\ud83d\udd39 Katman 11, Kafa 5:\n",
        "En anlaml\u0131 ve ak\u0131ll\u0131 ba\u011flant\u0131lar bu katmanda.\n",
        "\n",
        "\"Artificial\" ve \"intelligence\" kelimeleri birbirine \u00e7ok g\u00fc\u00e7l\u00fc ba\u011fl\u0131.\n",
        "\n",
        "Ayr\u0131ca c\u00fcmledeki uzak kelimeler aras\u0131nda da dikkat var (\u00f6rne\u011fin \"The\" \u2192 c\u00fcmle sonu).\n",
        "\n",
        "\ud83d\udcc8 Genel \u00d6zet:\n",
        "\u0130lk katmanlar: Her kelime sadece kendisine odaklan\u0131yor. (\ud83d\udccd Yerel dikkat)\n",
        "\n",
        "Orta katmanlar: Kelimeler aras\u0131nda daha fazla ba\u011flant\u0131 ba\u015fl\u0131yor. (\ud83d\udd17 Dilbilgisi ili\u015fkileri)\n",
        "\n",
        "Son katmanlar: Anlaml\u0131 kelimeler birbirini tan\u0131yor. (\ud83e\udde0 Anlam gruplama)\n",
        "\n",
        "Kafa 0: Daha d\u00fczenli ve klasik, \u00e7o\u011funlukla sadece diagonal.\n",
        "\n",
        "Kafa 5: Daha yarat\u0131c\u0131 ve esnek, kelimeler aras\u0131nda g\u00fc\u00e7l\u00fc ba\u011flant\u0131lar kuruyor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY-XOT4Tn5Qr"
      },
      "source": [
        "\u00d6zet: Kod, GPT-2 ve BERT modellerinin metin \u00fcretimi ve analiz performans\u0131n\u0131 \u00f6l\u00e7en metrikler (kelime say\u0131s\u0131, \u00e7e\u015fitlilik, h\u0131z, yarat\u0131c\u0131l\u0131k vb.) hesapl\u0131yor ve testin ba\u015far\u0131l\u0131 oldu\u011funu bildiriyor. Detayl\u0131 sonu\u00e7lar\u0131 g\u00f6rmek i\u00e7in test_metrics yazd\u0131r\u0131lmal\u0131."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 1250,
          "status": "ok",
          "timestamp": 1752826021129,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "M1xRnOXXknp-",
        "outputId": "387adadb-f918-4d2c-e78a-58cae77d6c28"
      },
      "outputs": [],
      "source": [
        "print_separator(\"G\u00f6rselle\u015ftirme Fonksiyonlar\u0131\")\n",
        "\n",
        "def create_performance_visualizations(comparison_results, metrics):\n",
        "    \"\"\"\n",
        "    Performans g\u00f6rselle\u015ftirmeleri olu\u015fturur\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('Model Kar\u015f\u0131la\u015ft\u0131rma Analizi', fontsize=16, fontweight='bold')\n",
        "\n",
        "    try:\n",
        "        # 1. Zaman kar\u015f\u0131la\u015ft\u0131rmas\u0131\n",
        "        if 'performance_metrics' in comparison_results:\n",
        "            times = [\n",
        "                comparison_results['performance_metrics'].get('gpt2_time', 0),\n",
        "                comparison_results['performance_metrics'].get('bert_time', 0)\n",
        "            ]\n",
        "            models = ['GPT-2', 'BERT']\n",
        "\n",
        "            axes[0, 0].bar(models, times, color=['#3498db', '#e74c3c'])\n",
        "            axes[0, 0].set_title('\u0130\u015flem S\u00fcreleri')\n",
        "            axes[0, 0].set_ylabel('S\u00fcre (saniye)')\n",
        "\n",
        "            # De\u011ferleri bar'lar\u0131n \u00fczerine yaz\n",
        "            for i, v in enumerate(times):\n",
        "                axes[0, 0].text(i, v + 0.001, f'{v:.3f}s', ha='center')\n",
        "\n",
        "        # 2. Kelime say\u0131s\u0131 kar\u015f\u0131la\u015ft\u0131rmas\u0131\n",
        "        if 'gpt2' in metrics and 'bert' in metrics:\n",
        "            word_counts = [\n",
        "                metrics['gpt2'].get('total_words', 0),\n",
        "                metrics['bert'].get('vocabulary_coverage', 0)\n",
        "            ]\n",
        "\n",
        "            axes[0, 1].bar(models, word_counts, color=['#2ecc71', '#f39c12'])\n",
        "            axes[0, 1].set_title('Kelime/Token Say\u0131s\u0131')\n",
        "            axes[0, 1].set_ylabel('Say\u0131')\n",
        "\n",
        "            for i, v in enumerate(word_counts):\n",
        "                axes[0, 1].text(i, v + 1, str(int(v)), ha='center')\n",
        "\n",
        "        # 3. Yarat\u0131c\u0131l\u0131k ve \u00e7e\u015fitlilik\n",
        "        if 'gpt2' in metrics and 'bert' in metrics:\n",
        "            creativity_scores = [\n",
        "                metrics['gpt2'].get('creativity_score', 0),\n",
        "                metrics['bert'].get('token_diversity', 0)\n",
        "            ]\n",
        "\n",
        "            axes[1, 0].bar(models, creativity_scores, color=['#9b59b6', '#1abc9c'])\n",
        "            axes[1, 0].set_title('Yarat\u0131c\u0131l\u0131k/\u00c7e\u015fitlilik Skoru')\n",
        "            axes[1, 0].set_ylabel('Skor (0-1)')\n",
        "            axes[1, 0].set_ylim(0, 1)\n",
        "\n",
        "            for i, v in enumerate(creativity_scores):\n",
        "                axes[1, 0].text(i, v + 0.02, f'{v:.3f}', ha='center')\n",
        "\n",
        "        # 4. Tekrar oran\u0131\n",
        "        if 'gpt2' in metrics:\n",
        "            repetition_rate = metrics['gpt2'].get('repetition_rate', 0)\n",
        "\n",
        "            axes[1, 1].pie([repetition_rate, 1-repetition_rate],\n",
        "                          labels=['Tekrar', '\u00d6zg\u00fcn'],\n",
        "                          colors=['#e67e22', '#27ae60'],\n",
        "                          autopct='%1.1f%%')\n",
        "            axes[1, 1].set_title('GPT-2 Tekrar Oran\u0131')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return fig\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c G\u00f6rselle\u015ftirme hatas\u0131: {e}\")\n",
        "        return None\n",
        "\n",
        "# Test g\u00f6rselle\u015ftirme\n",
        "print(\"\ud83e\uddea G\u00f6rselle\u015ftirme test ediliyor...\")\n",
        "if 'error' not in test_comparison and 'error' not in test_metrics:\n",
        "    test_viz = create_performance_visualizations(test_comparison, test_metrics)\n",
        "    if test_viz:\n",
        "        print(\"\u2705 G\u00f6rselle\u015ftirme ba\u015far\u0131l\u0131!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hgMjQ1mofwF"
      },
      "source": [
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "\n",
        "\u0130\u015flem S\u00fcreleri:\n",
        "GPT-2: ~0.947 saniye (mavi \u00e7ubuk).\n",
        "BERT: ~0.0275 saniye (k\u0131rm\u0131z\u0131 \u00e7ubuk).\n",
        "Yorum: BERT, GPT-2'ye k\u0131yasla \u00e7ok daha h\u0131zl\u0131 i\u015fliyor (yakla\u015f\u0131k 34 kat daha h\u0131zl\u0131).\n",
        "\n",
        "\n",
        "\n",
        "Kelime/Token Say\u0131s\u0131:\n",
        "GPT-2: 60 kelime (ye\u015fil \u00e7ubuk).\n",
        "BERT: 7 token (turuncu \u00e7ubuk).\n",
        "Yorum: GPT-2, BERT'e g\u00f6re \u00e7ok daha fazla kelime \u00fcretiyor, ancak BERT'in token say\u0131s\u0131 daha s\u0131n\u0131rl\u0131 (analiz odakl\u0131 oldu\u011fu i\u00e7in beklenen bir sonu\u00e7).\n",
        "\n",
        "\n",
        "Yarat\u0131c\u0131l\u0131k/\u00c7e\u015fitlilik Skoru:\n",
        "GPT-2: 0.900 (mor \u00e7ubuk).\n",
        "BERT: 1.000 (turkuaz \u00e7ubuk).\n",
        "Yorum: BERT, token \u00e7e\u015fitlili\u011fi a\u00e7\u0131s\u0131ndan maksimum skora sahipken, GPT-2 de y\u00fcksek bir yarat\u0131c\u0131l\u0131k skoru g\u00f6steriyor. BERT'in 1.0 skoru, token \u00e7e\u015fitlili\u011finin m\u00fckemmel oldu\u011funu ima eder.\n",
        "\n",
        "\n",
        "\n",
        "GPT-2 Tekrar Oran\u0131:\n",
        "\u00d6zg\u00fcn: %80 (ye\u015fil dilim).\n",
        "Tekrar: %20 (turuncu dilim).\n",
        "Yorum: GPT-2'nin \u00fcretti\u011fi metinlerin %80'i \u00f6zg\u00fcn, %20'si tekrar i\u00e7eriyor. Bu, GPT-2'nin olduk\u00e7a \u00e7e\u015fitli metinler \u00fcretti\u011fini, ancak k\u00fc\u00e7\u00fck bir tekrar oran\u0131na sahip oldu\u011funu g\u00f6steriyor.\n",
        "\n",
        "\n",
        "Genel De\u011ferlendirme\n",
        "H\u0131z: BERT, i\u015flem s\u00fcresinde a\u00e7\u0131k ara \u00f6nde.\n",
        "\u00dcretim Kapasitesi: GPT-2, kelime say\u0131s\u0131nda dominant.\n",
        "Kalite: BERT'in token \u00e7e\u015fitlili\u011fi m\u00fckemmel, GPT-2'nin yarat\u0131c\u0131l\u0131\u011f\u0131 da y\u00fcksek ve tekrar oran\u0131 d\u00fc\u015f\u00fck.\n",
        "G\u00f6rselle\u015ftirmeler, modellerin g\u00fc\u00e7l\u00fc ve zay\u0131f y\u00f6nlerini net bir \u015fekilde ortaya koyuyor. Test ba\u015far\u0131l\u0131 olmu\u015f ve grafikler, performans farklar\u0131n\u0131 g\u00f6rsel olarak destekliyor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1804,
          "status": "ok",
          "timestamp": 1752826035972,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "6VejOhEOkqFe",
        "outputId": "57bd3796-f69d-45e8-d77d-e8c7e5b879e5"
      },
      "outputs": [],
      "source": [
        "print_separator(\"Toplu Kar\u015f\u0131la\u015ft\u0131rma\")\n",
        "\n",
        "def batch_model_comparison(prompts, save_results=True):\n",
        "    \"\"\"\n",
        "    Birden fazla prompt i\u00e7in model kar\u015f\u0131la\u015ft\u0131rmas\u0131 yapar\n",
        "    \"\"\"\n",
        "    all_results = []\n",
        "\n",
        "    print(f\"\ud83d\udd04 {len(prompts)} prompt i\u00e7in kar\u015f\u0131la\u015ft\u0131rma ba\u015flat\u0131l\u0131yor...\")\n",
        "\n",
        "    for i, prompt in enumerate(tqdm(prompts, desc=\"Kar\u015f\u0131la\u015ft\u0131rma\")):\n",
        "        try:\n",
        "            # Her prompt i\u00e7in kar\u015f\u0131la\u015ft\u0131rma\n",
        "            result = detailed_model_comparison(\n",
        "                prompt=prompt,\n",
        "                max_length=40,\n",
        "                temperature=0.7,\n",
        "                num_sequences=2\n",
        "            )\n",
        "\n",
        "            if 'error' not in result:\n",
        "                metrics = calculate_performance_metrics(result)\n",
        "                result['metrics'] = metrics\n",
        "\n",
        "            all_results.append({\n",
        "                'prompt_index': i + 1,\n",
        "                'prompt': prompt,\n",
        "                'comparison': result\n",
        "            })\n",
        "\n",
        "            # GPU belle\u011fini temizle\n",
        "            clear_gpu_memory()\n",
        "\n",
        "        except Exception as e:\n",
        "            all_results.append({\n",
        "                'prompt_index': i + 1,\n",
        "                'prompt': prompt,\n",
        "                'error': str(e)\n",
        "            })\n",
        "\n",
        "    # \u00d6zet istatistikler\n",
        "    summary = create_batch_summary(all_results)\n",
        "\n",
        "    if save_results:\n",
        "        # Sonu\u00e7lar\u0131 DataFrame'e \u00e7evir\n",
        "        df_results = pd.DataFrame([\n",
        "            {\n",
        "                'Prompt': result['prompt'],\n",
        "                'GPT-2 S\u00fcresi': result['comparison'].get('performance_metrics', {}).get('gpt2_time', 0),\n",
        "                'BERT S\u00fcresi': result['comparison'].get('performance_metrics', {}).get('bert_time', 0),\n",
        "                'Hata': 'error' in result['comparison']\n",
        "            }\n",
        "            for result in all_results\n",
        "        ])\n",
        "\n",
        "        print(\"\\n\ud83d\udcca Toplu Kar\u015f\u0131la\u015ft\u0131rma \u00d6zeti:\")\n",
        "        print(df_results.to_string(index=False))\n",
        "\n",
        "    return all_results, summary\n",
        "\n",
        "def create_batch_summary(results):\n",
        "    \"\"\"\n",
        "    Toplu kar\u015f\u0131la\u015ft\u0131rma \u00f6zetini olu\u015fturur\n",
        "    \"\"\"\n",
        "    summary = {\n",
        "        'total_prompts': len(results),\n",
        "        'successful_comparisons': 0,\n",
        "        'failed_comparisons': 0,\n",
        "        'avg_gpt2_time': 0,\n",
        "        'avg_bert_time': 0,\n",
        "        'fastest_model': 'Unknown'\n",
        "    }\n",
        "\n",
        "    gpt2_times = []\n",
        "    bert_times = []\n",
        "\n",
        "    for result in results:\n",
        "        if 'error' not in result['comparison']:\n",
        "            summary['successful_comparisons'] += 1\n",
        "\n",
        "            perf_metrics = result['comparison'].get('performance_metrics', {})\n",
        "            gpt2_time = perf_metrics.get('gpt2_time', 0)\n",
        "            bert_time = perf_metrics.get('bert_time', 0)\n",
        "\n",
        "            if gpt2_time > 0:\n",
        "                gpt2_times.append(gpt2_time)\n",
        "            if bert_time > 0:\n",
        "                bert_times.append(bert_time)\n",
        "        else:\n",
        "            summary['failed_comparisons'] += 1\n",
        "\n",
        "    if gpt2_times:\n",
        "        summary['avg_gpt2_time'] = np.mean(gpt2_times)\n",
        "    if bert_times:\n",
        "        summary['avg_bert_time'] = np.mean(bert_times)\n",
        "\n",
        "    if summary['avg_gpt2_time'] > 0 and summary['avg_bert_time'] > 0:\n",
        "        summary['fastest_model'] = 'BERT' if summary['avg_bert_time'] < summary['avg_gpt2_time'] else 'GPT-2'\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Test toplu kar\u015f\u0131la\u015ft\u0131rma\n",
        "print(\"\ud83e\uddea Toplu kar\u015f\u0131la\u015ft\u0131rma test ediliyor...\")\n",
        "test_prompts = [\n",
        "    \"The future of technology is\",\n",
        "    \"Climate change will affect\",\n",
        "    \"Artificial intelligence can help\"\n",
        "]\n",
        "\n",
        "test_batch_results, test_summary = batch_model_comparison(test_prompts)\n",
        "print(f\"\u2705 {test_summary['successful_comparisons']}/{test_summary['total_prompts']} kar\u015f\u0131la\u015ft\u0131rma ba\u015far\u0131l\u0131!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 33,
          "status": "ok",
          "timestamp": 1752826041626,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "mIZ7PHNoks9m",
        "outputId": "effe35fd-c982-4ad9-bb08-8ad1f15acaf3"
      },
      "outputs": [],
      "source": [
        "print_separator(\"Model Kar\u015f\u0131la\u015ft\u0131rmas\u0131 \u00d6zeti\")\n",
        "\n",
        "comparison_summary = {\n",
        "    'Fonksiyon': [\n",
        "        'detailed_model_comparison',\n",
        "        'calculate_performance_metrics',\n",
        "        'create_performance_visualizations',\n",
        "        'generate_comparison_report',\n",
        "        'batch_model_comparison'\n",
        "    ],\n",
        "    'A\u00e7\u0131klama': [\n",
        "        'Detayl\u0131 model kar\u015f\u0131la\u015ft\u0131rmas\u0131',\n",
        "        'Performans metriklerini hesapla',\n",
        "        'G\u00f6rselle\u015ftirmeler olu\u015ftur',\n",
        "        'Detayl\u0131 rapor olu\u015ftur',\n",
        "        'Toplu kar\u015f\u0131la\u015ft\u0131rma yap'\n",
        "    ],\n",
        "    'Durum': [\n",
        "        '\u2705 Haz\u0131r',\n",
        "        '\u2705 Haz\u0131r',\n",
        "        '\u2705 Haz\u0131r',\n",
        "        '\u2705 Haz\u0131r',\n",
        "        '\u2705 Haz\u0131r'\n",
        "    ]\n",
        "}\n",
        "\n",
        "df_comparison = pd.DataFrame(comparison_summary)\n",
        "print(df_comparison.to_string(index=False))\n",
        "\n",
        "print(\"\\n\ud83c\udfaf Model Kar\u015f\u0131la\u015ft\u0131rmas\u0131 tamamland\u0131!\")\n",
        "print(\"\ud83d\udcca Performans metrikleri hesaplan\u0131yor\")\n",
        "print(\"\ud83d\udcc8 G\u00f6rselle\u015ftirmeler olu\u015fturuluyor\")\n",
        "print(\"\ud83d\udcc4 Detayl\u0131 raporlar haz\u0131rlan\u0131yor\")\n",
        "print(\"\ud83d\udce6 Toplu i\u015fleme deste\u011fi haz\u0131r\")\n",
        "\n",
        "print(\"\\n\ud83d\udd04 Bir sonraki ad\u0131m: Attention G\u00f6rselle\u015ftirme\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWdwJxpzkvw_"
      },
      "source": [
        "# **6-ATTENTION G\u00d6RSELLE\u015eTIRME**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 3672,
          "status": "ok",
          "timestamp": 1752826068363,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "rzLxb0iwkx4l",
        "outputId": "143b025d-aa68-441d-a615-7db8be129285"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# \u00d6nceki ad\u0131mlardan gelen print_separator fonksiyonu\n",
        "def print_separator(title=\"\"):\n",
        "    \"\"\"G\u00fczel bir ay\u0131r\u0131c\u0131 yazd\u0131r\u0131r\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"\ud83c\udfaf {title}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "# BERT model ve tokenizer'\u0131 kontrol et, gerekirse yeniden y\u00fckle\n",
        "try:\n",
        "    bert_model\n",
        "    bert_tokenizer\n",
        "except NameError:\n",
        "    print(\"\ud83d\udce5 BERT model ve tokenizer yeniden y\u00fckleniyor...\")\n",
        "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    bert_model = bert_model.to(device)\n",
        "    bert_model.eval()\n",
        "    print(\"\u2705 BERT yeniden y\u00fcklendi!\")\n",
        "\n",
        "print_separator(\"Attention G\u00f6rselle\u015ftirme\")\n",
        "\n",
        "def visualize_attention(prompt, layer=0, head=0, figsize=(10, 8)):\n",
        "    \"\"\"\n",
        "    BERT modelinin attention a\u011f\u0131rl\u0131klar\u0131n\u0131 g\u00f6rselle\u015ftirir\n",
        "\n",
        "    Args:\n",
        "        prompt: Giri\u015f metni\n",
        "        layer: G\u00f6rselle\u015ftirilecek katman (0-11)\n",
        "        head: G\u00f6rselle\u015ftirilecek kafa (0-11)\n",
        "        figsize: G\u00f6rsel boyutu\n",
        "    \"\"\"\n",
        "    if bert_model is None or bert_tokenizer is None:\n",
        "        return \"\u274c BERT modeli veya tokenizer y\u00fcklenmedi!\"\n",
        "\n",
        "    try:\n",
        "        # Giri\u015f kontrol\u00fc\n",
        "        if not prompt.strip():\n",
        "            return \"\u274c L\u00fctfen ge\u00e7erli bir metin girin!\"\n",
        "\n",
        "        # Metni tokenize et ve attention \u00e7\u0131kt\u0131s\u0131n\u0131 al\n",
        "        inputs = bert_tokenizer(\n",
        "            prompt,\n",
        "            return_tensors='pt',\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=512\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = bert_model(**inputs, output_attentions=True)\n",
        "\n",
        "        # Attention a\u011f\u0131rl\u0131klar\u0131n\u0131 al\n",
        "        attentions = outputs.attentions  # [num_layers, batch_size, num_heads, seq_length, seq_length]\n",
        "        if layer >= len(attentions) or head >= attentions[0].shape[2]:\n",
        "            return f\"\u274c Hata: Ge\u00e7ersiz katman ({layer}) veya kafa ({head}) indeksi!\"\n",
        "\n",
        "        attention_weights = attentions[layer][0][head].cpu().numpy()  # [seq_length, seq_length]\n",
        "\n",
        "        # Tokenlar\u0131 al\n",
        "        tokens = bert_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "        # G\u00f6rselle\u015ftirme\n",
        "        plt.figure(figsize=figsize)\n",
        "        sns.heatmap(\n",
        "            attention_weights,\n",
        "            xticklabels=tokens,\n",
        "            yticklabels=tokens,\n",
        "            cmap='viridis',\n",
        "            square=True,\n",
        "            cbar_kws={'label': 'Attention A\u011f\u0131rl\u0131\u011f\u0131'}\n",
        "        )\n",
        "\n",
        "        plt.title(f'BERT Attention - Katman {layer}, Kafa {head}')\n",
        "        plt.xlabel('Tokenlar (X)')\n",
        "        plt.ylabel('Tokenlar (Y)')\n",
        "\n",
        "        # Etiketleri d\u00f6nd\u00fcr\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.yticks(rotation=0)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # GPU belle\u011fini temizle\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        return plt.gcf()\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"\u274c Hata: {str(e)}\"\n",
        "\n",
        "def visualize_multiple_attentions(prompt, layers=[0, 5, 11], heads=[0, 5, 11], figsize=(15, 12)):\n",
        "    \"\"\"\n",
        "    Birden fazla katman ve kafa i\u00e7in attention g\u00f6rselle\u015ftirmesi yapar\n",
        "\n",
        "    Args:\n",
        "        prompt: Giri\u015f metni\n",
        "        layers: G\u00f6rselle\u015ftirilecek katmanlar\n",
        "        heads: G\u00f6rselle\u015ftirilecek kafalar\n",
        "        figsize: Toplam g\u00f6rsel boyutu\n",
        "    \"\"\"\n",
        "    if bert_model is None or bert_tokenizer is None:\n",
        "        return \"\u274c BERT modeli veya tokenizer y\u00fcklenmedi!\"\n",
        "\n",
        "    try:\n",
        "        # Giri\u015f kontrol\u00fc\n",
        "        if not prompt.strip():\n",
        "            return \"\u274c L\u00fctfen ge\u00e7erli bir metin girin!\"\n",
        "\n",
        "        # Metni tokenize et\n",
        "        inputs = bert_tokenizer(\n",
        "            prompt,\n",
        "            return_tensors='pt',\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=512\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = bert_model(**inputs, output_attentions=True)\n",
        "\n",
        "        # Attention a\u011f\u0131rl\u0131klar\u0131n\u0131 al\n",
        "        attentions = outputs.attentions\n",
        "        tokens = bert_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])  # D\u00fczeltildi: input_id \u2192 input_ids\n",
        "\n",
        "        # Katman ve kafa indekslerini kontrol et\n",
        "        max_layers = len(attentions)\n",
        "        max_heads = attentions[0].shape[2]\n",
        "        layers = [l for l in layers if 0 <= l < max_layers]\n",
        "        heads = [h for h in heads if 0 <= h < max_heads]\n",
        "        if not layers or not heads:\n",
        "            return f\"\u274c Hata: Ge\u00e7ersiz katman veya kafa indeksi! Maksimum katman: {max_layers}, Maksimum kafa: {max_heads}\"\n",
        "\n",
        "        # \u00c7oklu g\u00f6rselle\u015ftirme i\u00e7in subplot\n",
        "        fig, axes = plt.subplots(len(layers), len(heads), figsize=figsize)\n",
        "        fig.suptitle('BERT Attention A\u011f\u0131rl\u0131klar\u0131 - \u00c7oklu Katmanlar ve Kafalar', fontsize=16)\n",
        "\n",
        "        for i, layer in enumerate(layers):\n",
        "            for j, head in enumerate(heads):\n",
        "                attention_weights = attentions[layer][0][head].cpu().numpy()\n",
        "\n",
        "                if len(layers) == 1 and len(heads) == 1:\n",
        "                    ax = axes\n",
        "                elif len(layers) == 1:\n",
        "                    ax = axes[j]\n",
        "                elif len(heads) == 1:\n",
        "                    ax = axes[i]\n",
        "                else:\n",
        "                    ax = axes[i, j]\n",
        "\n",
        "                sns.heatmap(\n",
        "                    attention_weights,\n",
        "                    xticklabels=tokens,\n",
        "                    yticklabels=tokens,\n",
        "                    cmap='viridis',\n",
        "                    square=True,\n",
        "                    cbar_kws={'label': 'A\u011f\u0131rl\u0131k'},\n",
        "                    ax=ax\n",
        "                )\n",
        "\n",
        "                ax.set_title(f'Katman {layer}, Kafa {head}')\n",
        "                ax.set_xlabel('Tokenlar (X)')\n",
        "                ax.set_ylabel('Tokenlar (Y)')\n",
        "                ax.tick_params(axis='x', rotation=45, labelright=True)\n",
        "                ax.tick_params(axis='y', rotation=0)\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "\n",
        "        # GPU belle\u011fini temizle\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        return fig\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"\u274c Hata: {str(e)}\"\n",
        "\n",
        "# Test fonksiyonlar\u0131\n",
        "print(\"\ud83e\uddea Attention g\u00f6rselle\u015ftirme test ediliyor...\")\n",
        "test_prompt = \"The future of artificial intelligence is bright\"\n",
        "\n",
        "# Tek katman ve kafa i\u00e7in test\n",
        "single_attention_fig = visualize_attention(test_prompt, layer=0, head=0)\n",
        "if isinstance(single_attention_fig, plt.Figure):\n",
        "    print(\"\u2705 Tek attention g\u00f6rselle\u015ftirme ba\u015far\u0131l\u0131!\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f\"\u274c Tek attention hatas\u0131: {single_attention_fig}\")\n",
        "\n",
        "# Birden fazla katman ve kafa i\u00e7in test\n",
        "multiple_attention_fig = visualize_multiple_attentions(test_prompt, layers=[0, 5, 11], heads=[0, 5, 11])\n",
        "if isinstance(multiple_attention_fig, plt.Figure):\n",
        "    print(\"\u2705 \u00c7oklu attention g\u00f6rselle\u015ftirme ba\u015far\u0131l\u0131!\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f\"\u274c \u00c7oklu attention hatas\u0131: {multiple_attention_fig}\")\n",
        "\n",
        "# \u00d6zet tablo\n",
        "print_separator(\"Attention G\u00f6rselle\u015ftirme \u00d6zeti\")\n",
        "attention_summary = {\n",
        "    'Fonksiyon': ['visualize_attention', 'visualize_multiple_attentions'],\n",
        "    'A\u00e7\u0131klama': ['Tek katman/kafa i\u00e7in attention g\u00f6rselle\u015ftirme', 'Birden fazla katman/kafa i\u00e7in attention g\u00f6rselle\u015ftirme'],\n",
        "    'Durum': ['\u2705 Haz\u0131r', '\u2705 Haz\u0131r']\n",
        "}\n",
        "df_attention = pd.DataFrame(attention_summary)\n",
        "print(df_attention.to_string(index=False))\n",
        "print(\"\\n\ud83c\udfaf Attention g\u00f6rselle\u015ftirme tamamland\u0131!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW0yIbVjxlPk"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTkWh4QFwv0-"
      },
      "source": [
        "Bu kod, BERT modelinin attention mekanizmas\u0131n\u0131 g\u00f6rselle\u015ftirerek metin analizinde hangi kelimelerin birbirine ne kadar \u00f6nem verdi\u011fini g\u00f6sterir. \u00c7\u0131kt\u0131lar, heatmap format\u0131nda dikkat a\u011f\u0131rl\u0131klar\u0131n\u0131 sunar ve ara\u015ft\u0131rmac\u0131lar\u0131n veya geli\u015ftiricilerin modelin i\u00e7 i\u015fleyi\u015fini anlamas\u0131na yard\u0131mc\u0131 olur. Test prompt'u \"The future of artificial intelligence is bright\" i\u00e7in olu\u015fturulan grafikler, farkl\u0131 katmanlarda ve kafalarda attention da\u011f\u0131l\u0131m\u0131n\u0131 detayl\u0131 bir \u015fekilde analiz etmeyi sa\u011flar.\n",
        "\n",
        "\ud83d\udd39 Layer (Katman):\n",
        "BERT'in d\u00fc\u015f\u00fcnme ad\u0131mlar\u0131 gibi. Her katman dili biraz daha iyi anlar.\n",
        "Layer 0 = en y\u00fczeysel, ilk bak\u0131\u015f.\n",
        "\n",
        "\ud83d\udd39 Head (Kafa):\n",
        "Her katmanda birden fazla dikkat \"bak\u0131\u015f\u0131\" vard\u0131r.\n",
        "Head 0 = o katmandaki ilk dikkat penceresi. Her biri farkl\u0131 ili\u015fkilere odaklan\u0131r.\n",
        "\n",
        "****************************************************************************\n",
        "\n",
        "Grafik 1 \u00d6zeti (BERT layer 0, head 0):\n",
        "C\u00fcmle: \"The future of artificial intelligence is bright\"\n",
        "\n",
        "BERT modelinin ilk katman\u0131ndaki dikkat (attention) haritas\u0131 incelenmi\u015f. Modelin kelimelere nas\u0131l dikkat etti\u011fini g\u00f6steriyor.\n",
        "Ana G\u00f6zlemler:\n",
        "\n",
        "[CLS] \u2192 \"artificial\" ve \"intelligence\":\n",
        "Modelin \u00f6zel ba\u015flang\u0131\u00e7 sembol\u00fc olan [CLS], en \u00e7ok \"artificial\" ve \"intelligence\" kelimelerine odaklan\u0131yor. Bu sar\u0131 renkle g\u00f6sterilmi\u015f (y\u00fcksek dikkat).\n",
        "\n",
        "\"the\" \u2192 \"future\":\n",
        "\"the\" kelimesi en \u00e7ok \"future\" kelimesine bak\u0131yor. Bu orta d\u00fczey bir dikkat (ye\u015fil tonlarda).\n",
        "\n",
        "\"bright\" \u2192 \"[SEP]\":\n",
        "C\u00fcmle sonundaki \"bright\", c\u00fcmle biti\u015f i\u015faretine ([SEP]) d\u00fc\u015f\u00fck dikkat veriyor. Bu da mor renkli (d\u00fc\u015f\u00fck dikkat).\n",
        "\n",
        "*******************************************************************************\n",
        "\n",
        "\n",
        "GRAF\u0130K 2:\n",
        "BERT modelinin farkl\u0131 katman ve kafa kombinasyonlar\u0131nda attention a\u011f\u0131rl\u0131klar\u0131n\u0131 (dikkat da\u011f\u0131l\u0131m\u0131) g\u00f6steriyor. \u0130\u015fte k\u0131sa bir analiz:\n",
        "\n",
        "## Temel G\u00f6zlemler:\n",
        "\n",
        "\u2022 **Renk Skalas\u0131**: Koyu mor (d\u00fc\u015f\u00fck dikkat) - Sar\u0131 (y\u00fcksek dikkat) aras\u0131nda de\u011fi\u015fiyor\n",
        "\n",
        "\u2022 **Token Analizi**: \"The future of artificial intelligence is bright\" c\u00fcmlesinin her kelimesi i\u00e7in dikkat da\u011f\u0131l\u0131m\u0131 g\u00f6steriliyor\n",
        "\n",
        "\u2022 **Katman Farklar\u0131**:\n",
        "  - **Katman 0**: Daha d\u00fc\u015f\u00fck dikkat de\u011ferleri (0.35 max)\n",
        "  - **Katman 5**: Orta seviye dikkat (0.4 max)\n",
        "  - **Katman 11**: En y\u00fcksek dikkat de\u011ferleri (0.8 max)\n",
        "\n",
        "## Dikkat Da\u011f\u0131l\u0131m\u0131 Desenleri:\n",
        "\n",
        "\u2022 **Erken Katmanlar**: Daha da\u011f\u0131n\u0131k, d\u00fc\u015f\u00fck yo\u011funluklu dikkat\n",
        "\u2022 **Ge\u00e7 Katmanlar**: Daha odakl\u0131, y\u00fcksek yo\u011funluklu dikkat\n",
        "\u2022 **\u00d6zel Tokenlar**: [CLS] ve [SEP] tokenlar\u0131na farkl\u0131 dikkat da\u011f\u0131l\u0131mlar\u0131\n",
        "\n",
        "## Kafa Farkl\u0131l\u0131klar\u0131:\n",
        "\n",
        "\u2022 **Kafa 0**: Genel olarak daha d\u00fc\u015f\u00fck dikkat de\u011ferleri\n",
        "\u2022 **Kafa 5**: Daha y\u00fcksek ve odakl\u0131 dikkat da\u011f\u0131l\u0131m\u0131\n",
        "\n",
        "Bu analiz, BERT'in derin katmanlarda daha spesifik ve g\u00fc\u00e7l\u00fc dikkat mekanizmalar\u0131 geli\u015ftirdi\u011fini g\u00f6steriyor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikYIGXDQk2AJ"
      },
      "source": [
        "# **7-TEST VE ANALIZ**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 3581,
          "status": "ok",
          "timestamp": 1752826109128,
          "user": {
            "displayName": "Baha \u00d6zk\u00fcz",
            "userId": "05851936510626901424"
          },
          "user_tz": -180
        },
        "id": "FwyNnkCZk8gZ",
        "outputId": "4953f21f-4fa3-4617-893e-b97083862e50"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, BertTokenizer, BertModel\n",
        "import io\n",
        "from PIL import Image\n",
        "\n",
        "# \u00d6nceki ad\u0131mlardan gelen fonksiyonlar\n",
        "def print_separator(title=\"\"):\n",
        "    \"\"\"G\u00fczel bir ay\u0131r\u0131c\u0131 yazd\u0131r\u0131r\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"\ud83c\udfaf {title}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "def generate_text_gpt2(prompt, max_length=50, num_return_sequences=1, temperature=0.7, top_p=0.9, do_sample=True):\n",
        "    \"\"\"GPT-2 ile metin tamamlama\"\"\"\n",
        "    if gpt2_model is None or gpt2_tokenizer is None:\n",
        "        return \"\u274c GPT-2 modeli y\u00fcklenmedi!\"\n",
        "    try:\n",
        "        inputs = gpt2_tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "        if inputs.shape[1] > 1000:\n",
        "            return \"\u274c Giri\u015f metni \u00e7ok uzun! (Max 1000 token)\"\n",
        "        with torch.no_grad():\n",
        "            outputs = gpt2_model.generate(\n",
        "                inputs,\n",
        "                max_length=max_length,\n",
        "                num_return_sequences=num_return_sequences,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p,\n",
        "                do_sample=do_sample,\n",
        "                pad_token_id=gpt2_tokenizer.eos_token_id,\n",
        "                attention_mask=torch.ones(inputs.shape).to(device)\n",
        "            )\n",
        "        results = []\n",
        "        for i, output in enumerate(outputs):\n",
        "            generated_text = gpt2_tokenizer.decode(output, skip_special_tokens=True)\n",
        "            results.append({\n",
        "                'sequence': i + 1,\n",
        "                'text': generated_text,\n",
        "                'new_text': generated_text[len(prompt):].strip()\n",
        "            })\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        return f\"\u274c Hata: {str(e)}\"\n",
        "\n",
        "def analyze_text_bert(text, return_attention=False):\n",
        "    \"\"\"BERT ile metin analizi\"\"\"\n",
        "    if bert_model is None or bert_tokenizer is None:\n",
        "        return \"\u274c BERT modeli y\u00fcklenmedi!\"\n",
        "    try:\n",
        "        inputs = bert_tokenizer(\n",
        "            text,\n",
        "            return_tensors='pt',\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=512\n",
        "        ).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = bert_model(**inputs, output_attentions=return_attention)\n",
        "        last_hidden_states = outputs.last_hidden_state\n",
        "        cls_embedding = last_hidden_states[:, 0, :]\n",
        "        token_embeddings = last_hidden_states[0]\n",
        "        tokens = bert_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "        result = {\n",
        "            'tokens': tokens,\n",
        "            'token_count': len(tokens),\n",
        "            'cls_embedding': cls_embedding.cpu().numpy(),\n",
        "            'token_embeddings': token_embeddings.cpu().numpy(),\n",
        "            'embedding_dim': last_hidden_states.shape[-1]\n",
        "        }\n",
        "        if return_attention:\n",
        "            result['attention_weights'] = outputs.attentions\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        return f\"\u274c Hata: {str(e)}\"\n",
        "\n",
        "def detailed_model_comparison(prompt, max_length=50, temperature=0.7, num_sequences=3):\n",
        "    \"\"\"GPT-2 ve BERT modellerini detayl\u0131 kar\u015f\u0131la\u015ft\u0131r\u0131r\"\"\"\n",
        "    comparison_results = {\n",
        "        'prompt': prompt,\n",
        "        'gpt2_results': [],\n",
        "        'bert_results': {},\n",
        "        'performance_metrics': {},\n",
        "        'analysis': {}\n",
        "    }\n",
        "    try:\n",
        "        print(\"\ud83d\udd04 GPT-2 ile metin \u00fcretiliyor...\")\n",
        "        start_time = time.time()\n",
        "        gpt2_outputs = generate_text_gpt2(\n",
        "            prompt=prompt,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            num_return_sequences=num_sequences\n",
        "        )\n",
        "        gpt2_time = time.time() - start_time\n",
        "        if isinstance(gpt2_outputs, list):\n",
        "            comparison_results['gpt2_results'] = gpt2_outputs\n",
        "            comparison_results['performance_metrics']['gpt2_time'] = gpt2_time\n",
        "            comparison_results['performance_metrics']['gpt2_sequences'] = len(gpt2_outputs)\n",
        "        print(\"\ud83d\udd04 BERT ile analiz yap\u0131l\u0131yor...\")\n",
        "        start_time = time.time()\n",
        "        bert_output = analyze_text_bert(prompt, return_attention=True)\n",
        "        bert_time = time.time() - start_time\n",
        "        if isinstance(bert_output, dict):\n",
        "            comparison_results['bert_results'] = bert_output\n",
        "            comparison_results['performance_metrics']['bert_time'] = bert_time\n",
        "        comparison_results['analysis'] = analyze_model_performance(\n",
        "            gpt2_outputs, bert_output, gpt2_time, bert_time\n",
        "        )\n",
        "        return comparison_results\n",
        "    except Exception as e:\n",
        "        comparison_results['error'] = str(e)\n",
        "        return comparison_results\n",
        "\n",
        "def analyze_model_performance(gpt2_results, bert_results, gpt2_time, bert_time):\n",
        "    \"\"\"Model performans\u0131n\u0131 analiz eder\"\"\"\n",
        "    analysis = {}\n",
        "    try:\n",
        "        if isinstance(gpt2_results, list) and len(gpt2_results) > 0:\n",
        "            generated_texts = [result['new_text'] for result in gpt2_results]\n",
        "            analysis['gpt2'] = {\n",
        "                'avg_length': np.mean([len(text.split()) for text in generated_texts]),\n",
        "                'max_length': max([len(text.split()) for text in generated_texts]),\n",
        "                'min_length': min([len(text.split()) for text in generated_texts]),\n",
        "                'generation_time': gpt2_time,\n",
        "                'words_per_second': sum([len(text.split()) for text in generated_texts]) / gpt2_time if gpt2_time > 0 else 0\n",
        "            }\n",
        "        if isinstance(bert_results, dict):\n",
        "            analysis['bert'] = {\n",
        "                'token_count': bert_results.get('token_count', 0),\n",
        "                'embedding_dim': bert_results.get('embedding_dim', 0),\n",
        "                'processing_time': bert_time,\n",
        "                'tokens_per_second': bert_results.get('token_count', 0) / bert_time if bert_time > 0 else 0\n",
        "            }\n",
        "        if 'gpt2' in analysis and 'bert' in analysis:\n",
        "            analysis['comparison'] = {\n",
        "                'speed_ratio': bert_time / gpt2_time if gpt2_time > 0 else 0,\n",
        "                'bert_faster': bert_time < gpt2_time,\n",
        "                'gpt2_creativity': analysis['gpt2']['avg_length'] > 10,\n",
        "                'bert_understanding': analysis['bert']['token_count'] > 5\n",
        "            }\n",
        "        return analysis\n",
        "    except Exception as e:\n",
        "        return {'error': str(e)}\n",
        "\n",
        "def calculate_performance_metrics(comparison_results):\n",
        "    \"\"\"Performans metriklerini hesaplar\"\"\"\n",
        "    metrics = {}\n",
        "    try:\n",
        "        if 'gpt2_results' in comparison_results and comparison_results['gpt2_results']:\n",
        "            gpt2_texts = [result['new_text'] for result in comparison_results['gpt2_results']]\n",
        "            metrics['gpt2'] = {\n",
        "                'total_words': sum([len(text.split()) for text in gpt2_texts]),\n",
        "                'avg_word_length': np.mean([np.mean([len(word) for word in text.split()]) for text in gpt2_texts if text.split()]),\n",
        "                'unique_words': len(set(' '.join(gpt2_texts).split())),\n",
        "                'repetition_rate': calculate_repetition_rate(gpt2_texts),\n",
        "                'creativity_score': calculate_creativity_score(gpt2_texts)\n",
        "            }\n",
        "        if 'bert_results' in comparison_results and comparison_results['bert_results']:\n",
        "            bert_data = comparison_results['bert_results']\n",
        "            metrics['bert'] = {\n",
        "                'vocabulary_coverage': bert_data.get('token_count', 0),\n",
        "                'embedding_density': bert_data.get('embedding_dim', 0),\n",
        "                'token_diversity': calculate_token_diversity(bert_data.get('tokens', [])),\n",
        "                'processing_efficiency': bert_data.get('token_count', 0) / comparison_results['performance_metrics'].get('bert_time', 1)\n",
        "            }\n",
        "        return metrics\n",
        "    except Exception as e:\n",
        "        return {'error': str(e)}\n",
        "\n",
        "def calculate_repetition_rate(texts):\n",
        "    \"\"\"Tekrar oran\u0131n\u0131 hesaplar\"\"\"\n",
        "    if not texts:\n",
        "        return 0\n",
        "    all_words = ' '.join(texts).split()\n",
        "    unique_words = set(all_words)\n",
        "    return 1 - (len(unique_words) / len(all_words)) if all_words else 0\n",
        "\n",
        "def calculate_creativity_score(texts):\n",
        "    \"\"\"Yarat\u0131c\u0131l\u0131k skorunu hesaplar\"\"\"\n",
        "    if not texts:\n",
        "        return 0\n",
        "    all_words = ' '.join(texts).split()\n",
        "    unique_words = set(all_words)\n",
        "    avg_sentence_length = np.mean([len(text.split()) for text in texts])\n",
        "    diversity_score = len(unique_words) / len(all_words) if all_words else 0\n",
        "    length_score = min(avg_sentence_length / 20, 1)\n",
        "    return (diversity_score + length_score) / 2\n",
        "\n",
        "def calculate_token_diversity(tokens):\n",
        "    \"\"\"Token \u00e7e\u015fitlili\u011fini hesaplar\"\"\"\n",
        "    if not tokens:\n",
        "        return 0\n",
        "    filtered_tokens = [token for token in tokens if not token.startswith('[') and not token.startswith('#')]\n",
        "    return len(set(filtered_tokens)) / len(filtered_tokens) if filtered_tokens else 0\n",
        "\n",
        "def visualize_attention(prompt, layer=0, head=0, figsize=(10, 8)):\n",
        "    \"\"\"BERT attention g\u00f6rselle\u015ftirme\"\"\"\n",
        "    if bert_model is None or bert_tokenizer is None:\n",
        "        return \"\u274c BERT modeli veya tokenizer y\u00fcklenmedi!\"\n",
        "    try:\n",
        "        if not prompt.strip():\n",
        "            return \"\u274c L\u00fctfen ge\u00e7erli bir metin girin!\"\n",
        "        inputs = bert_tokenizer(\n",
        "            prompt,\n",
        "            return_tensors='pt',\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=512\n",
        "        ).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = bert_model(**inputs, output_attentions=True)\n",
        "        attentions = outputs.attentions\n",
        "        if layer >= len(attentions) or head >= attentions[0].shape[2]:\n",
        "            return f\"\u274c Hata: Ge\u00e7ersiz katman ({layer}) veya kafa ({head}) indeksi!\"\n",
        "        attention_weights = attentions[layer][0][head].cpu().numpy()\n",
        "        tokens = bert_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "        plt.figure(figsize=figsize)\n",
        "        sns.heatmap(\n",
        "            attention_weights,\n",
        "            xticklabels=tokens,\n",
        "            yticklabels=tokens,\n",
        "            cmap='viridis',\n",
        "            square=True,\n",
        "            cbar_kws={'label': 'Attention A\u011f\u0131rl\u0131\u011f\u0131'}\n",
        "        )\n",
        "        plt.title(f'BERT Attention - Katman {layer}, Kafa {head}')\n",
        "        plt.xlabel('Tokenlar (X)')\n",
        "        plt.ylabel('Tokenlar (Y)')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.yticks(rotation=0)\n",
        "        plt.tight_layout()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        return plt.gcf()\n",
        "    except Exception as e:\n",
        "        return f\"\u274c Hata: {str(e)}\"\n",
        "\n",
        "# BERT ve GPT-2 modellerini kontrol et, gerekirse yeniden y\u00fckle\n",
        "try:\n",
        "    gpt2_model\n",
        "    gpt2_tokenizer\n",
        "    bert_model\n",
        "    bert_tokenizer\n",
        "except NameError:\n",
        "    print(\"\ud83d\udce5 Modeller yeniden y\u00fckleniyor...\")\n",
        "    gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
        "    gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    gpt2_model = gpt2_model.to(device)\n",
        "    bert_model = bert_model.to(device)\n",
        "    gpt2_model.eval()\n",
        "    bert_model.eval()\n",
        "    print(\"\u2705 Modeller yeniden y\u00fcklendi!\")\n",
        "\n",
        "print_separator(\"Test ve Analiz\")\n",
        "\n",
        "# Test senaryolar\u0131\n",
        "test_prompts = [\n",
        "    \"The future of artificial intelligence is\",\n",
        "    \"Climate change affects the world by\",\n",
        "    \"In the digital age, privacy is\",\n",
        "    \"Machine learning can revolutionize\"\n",
        "]\n",
        "\n",
        "# Test fonksiyonu\n",
        "def run_tests(prompts, max_length=40, temperature=0.7, num_sequences=2):\n",
        "    \"\"\"Modellerin performans\u0131n\u0131 test eder ve analiz yapar\"\"\"\n",
        "    test_results = []\n",
        "    print(\"\ud83d\udd04 Testler ba\u015flat\u0131l\u0131yor...\")\n",
        "    for prompt in tqdm(prompts, desc=\"Test \u0130\u015fleniyor\"):\n",
        "        try:\n",
        "            # Detayl\u0131 kar\u015f\u0131la\u015ft\u0131rma\n",
        "            comparison = detailed_model_comparison(\n",
        "                prompt=prompt,\n",
        "                max_length=max_length,\n",
        "                temperature=temperature,\n",
        "                num_sequences=num_sequences\n",
        "            )\n",
        "            if 'error' not in comparison:\n",
        "                # Performans metrikleri\n",
        "                metrics = calculate_performance_metrics(comparison)\n",
        "                # Attention g\u00f6rselle\u015ftirme\n",
        "                attention_fig = visualize_attention(prompt, layer=0, head=0)\n",
        "                # Rapor olu\u015ftur\n",
        "                report = generate_comparison_report(comparison, metrics)\n",
        "                test_results.append({\n",
        "                    'prompt': prompt,\n",
        "                    'comparison': comparison,\n",
        "                    'metrics': metrics,\n",
        "                    'attention_fig': attention_fig,\n",
        "                    'report': report\n",
        "                })\n",
        "                # GPU belle\u011fini temizle\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "            else:\n",
        "                test_results.append({\n",
        "                    'prompt': prompt,\n",
        "                    'error': comparison['error']\n",
        "                })\n",
        "        except Exception as e:\n",
        "            test_results.append({\n",
        "                'prompt': prompt,\n",
        "                'error': str(e)\n",
        "            })\n",
        "    return test_results\n",
        "\n",
        "# \u00d6zet analizi\n",
        "def analyze_test_results(test_results):\n",
        "    \"\"\"Test sonu\u00e7lar\u0131n\u0131 analiz eder ve \u00f6zetler\"\"\"\n",
        "    summary = {\n",
        "        'total_tests': len(test_results),\n",
        "        'successful_tests': 0,\n",
        "        'failed_tests': 0,\n",
        "        'avg_gpt2_time': 0,\n",
        "        'avg_bert_time': 0,\n",
        "        'avg_gpt2_words': 0,\n",
        "        'avg_bert_tokens': 0,\n",
        "        'performance_comparison': {}\n",
        "    }\n",
        "    gpt2_times = []\n",
        "    bert_times = []\n",
        "    gpt2_words = []\n",
        "    bert_tokens = []\n",
        "    for result in test_results:\n",
        "        if 'error' not in result:\n",
        "            summary['successful_tests'] += 1\n",
        "            gpt2_time = result['comparison']['performance_metrics'].get('gpt2_time', 0)\n",
        "            bert_time = result['comparison']['performance_metrics'].get('bert_time', 0)\n",
        "            gpt2_words_count = result['metrics']['gpt2'].get('total_words', 0)\n",
        "            bert_token_count = result['metrics']['bert'].get('vocabulary_coverage', 0)\n",
        "            if gpt2_time > 0:\n",
        "                gpt2_times.append(gpt2_time)\n",
        "            if bert_time > 0:\n",
        "                bert_times.append(bert_time)\n",
        "            if gpt2_words_count > 0:\n",
        "                gpt2_words.append(gpt2_words_count)\n",
        "            if bert_token_count > 0:\n",
        "                bert_tokens.append(bert_token_count)\n",
        "        else:\n",
        "            summary['failed_tests'] += 1\n",
        "    summary['avg_gpt2_time'] = np.mean(gpt2_times) if gpt2_times else 0\n",
        "    summary['avg_bert_time'] = np.mean(bert_times) if bert_times else 0\n",
        "    summary['avg_gpt2_words'] = np.mean(gpt2_words) if gpt2_words else 0\n",
        "    summary['avg_bert_tokens'] = np.mean(bert_tokens) if bert_tokens else 0\n",
        "    summary['performance_comparison'] = {\n",
        "        'faster_model': 'BERT' if summary['avg_bert_time'] < summary['avg_gpt2_time'] else 'GPT-2',\n",
        "        'speed_ratio': summary['avg_bert_time'] / summary['avg_gpt2_time'] if summary['avg_gpt2_time'] > 0 else 0\n",
        "    }\n",
        "    return summary\n",
        "\n",
        "# G\u00f6rselle\u015ftirme fonksiyonu\n",
        "def visualize_test_results(test_results, summary):\n",
        "    \"\"\"Test sonu\u00e7lar\u0131n\u0131 g\u00f6rselle\u015ftirir\"\"\"\n",
        "    try:\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "        fig.suptitle('Test ve Analiz Sonu\u00e7lar\u0131', fontsize=16, fontweight='bold')\n",
        "        # 1. Test ba\u015far\u0131 oran\u0131\n",
        "        success_rate = summary['successful_tests'] / summary['total_tests'] * 100\n",
        "        axes[0, 0].pie(\n",
        "            [success_rate, 100 - success_rate],\n",
        "            labels=['Ba\u015far\u0131l\u0131', 'Ba\u015far\u0131s\u0131z'],\n",
        "            colors=['#2ecc71', '#e74c3c'],\n",
        "            autopct='%1.1f%%'\n",
        "        )\n",
        "        axes[0, 0].set_title('Test Ba\u015far\u0131 Oran\u0131')\n",
        "        # 2. \u0130\u015flem s\u00fcreleri\n",
        "        models = ['GPT-2', 'BERT']\n",
        "        times = [summary['avg_gpt2_time'], summary['avg_bert_time']]\n",
        "        axes[0, 1].bar(models, times, color=['#3498db', '#f39c12'])\n",
        "        axes[0, 1].set_title('Ortalama \u0130\u015flem S\u00fcreleri')\n",
        "        axes[0, 1].set_ylabel('S\u00fcre (saniye)')\n",
        "        for i, v in enumerate(times):\n",
        "            axes[0, 1].text(i, v + 0.01, f'{v:.3f}s', ha='center')\n",
        "        # 3. Kelime/Token say\u0131lar\u0131\n",
        "        counts = [summary['avg_gpt2_words'], summary['avg_bert_tokens']]\n",
        "        axes[1, 0].bar(models, counts, color=['#9b59b6', '#1abc9c'])\n",
        "        axes[1, 0].set_title('Ortalama Kelime/Token Say\u0131s\u0131')\n",
        "        axes[1, 0].set_ylabel('Say\u0131')\n",
        "        for i, v in enumerate(counts):\n",
        "            axes[1, 0].text(i, v + 0.5, f'{v:.1f}', ha='center')\n",
        "        # 4. Attention \u00f6rne\u011fi\n",
        "        for result in test_results:\n",
        "            if 'attention_fig' in result and isinstance(result['attention_fig'], plt.Figure):\n",
        "                # Figure'\u00fc kaydet ve yeniden y\u00fckle\n",
        "                buf = io.BytesIO()\n",
        "                result['attention_fig'].savefig(buf, format='png')\n",
        "                buf.seek(0)\n",
        "                img = Image.open(buf)\n",
        "                axes[1, 1].imshow(img)\n",
        "                axes[1, 1].set_title('\u00d6rnek Attention G\u00f6rselle\u015ftirmesi')\n",
        "                axes[1, 1].axis('off')\n",
        "                buf.close()\n",
        "                break\n",
        "        else:\n",
        "            axes[1, 1].text(0.5, 0.5, 'Attention G\u00f6rselle\u015ftirme Bulunamad\u0131',\n",
        "                           ha='center', va='center')\n",
        "            axes[1, 1].set_title('\u00d6rnek Attention G\u00f6rselle\u015ftirmesi')\n",
        "            axes[1, 1].axis('off')\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c G\u00f6rselle\u015ftirme hatas\u0131: {e}\")\n",
        "        return None\n",
        "\n",
        "# Nihai raporlama\n",
        "def generate_final_report(test_results, summary):\n",
        "    \"\"\"Nihai test ve analiz raporunu olu\u015fturur\"\"\"\n",
        "    report = []\n",
        "    try:\n",
        "        report.append(\"# \ud83d\udcca Nihai Test ve Analiz Raporu\")\n",
        "        report.append(f\"**Test Tarihi:** {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        report.append(f\"**Toplam Test Say\u0131s\u0131:** {summary['total_tests']}\")\n",
        "        report.append(f\"**Ba\u015far\u0131l\u0131 Testler:** {summary['successful_tests']}\")\n",
        "        report.append(f\"**Ba\u015far\u0131s\u0131z Testler:** {summary['failed_tests']}\")\n",
        "        report.append(\"\\n---\\n\")\n",
        "        report.append(\"## \ud83d\udcc8 Genel Performans \u00d6zeti\")\n",
        "        report.append(f\"- **Ortalama GPT-2 \u0130\u015flem S\u00fcresi:** {summary['avg_gpt2_time']:.3f} saniye\")\n",
        "        report.append(f\"- **Ortalama BERT \u0130\u015flem S\u00fcresi:** {summary['avg_bert_time']:.3f} saniye\")\n",
        "        report.append(f\"- **Daha H\u0131zl\u0131 Model:** {summary['performance_comparison']['faster_model']}\")\n",
        "        report.append(f\"- **H\u0131z Oran\u0131 (BERT/GPT-2):** {summary['performance_comparison']['speed_ratio']:.2f}\")\n",
        "        report.append(f\"- **Ortalama GPT-2 Kelime Say\u0131s\u0131:** {summary['avg_gpt2_words']:.1f}\")\n",
        "        report.append(f\"- **Ortalama BERT Token Say\u0131s\u0131:** {summary['avg_bert_tokens']:.1f}\")\n",
        "        report.append(\"\\n---\\n\")\n",
        "        report.append(\"## \ud83d\udd0d Test Sonu\u00e7lar\u0131\")\n",
        "        for i, result in enumerate(test_results):\n",
        "            report.append(f\"### Test {i+1}: {result['prompt']}\")\n",
        "            if 'error' in result:\n",
        "                report.append(f\"**Hata:** {result['error']}\")\n",
        "            else:\n",
        "                report.append(f\"**GPT-2 Sonu\u00e7lar\u0131:**\")\n",
        "                for j, gpt2_result in enumerate(result['comparison']['gpt2_results']):\n",
        "                    report.append(f\"- Sonu\u00e7 {j+1}: {gpt2_result['text']}\")\n",
        "                report.append(f\"**BERT Token Say\u0131s\u0131:** {result['metrics']['bert']['vocabulary_coverage']}\")\n",
        "                report.append(f\"**BERT Embedding Boyutu:** {result['metrics']['bert']['embedding_density']}\")\n",
        "                report.append(f\"**Yarat\u0131c\u0131l\u0131k Skoru (GPT-2):** {result['metrics']['gpt2']['creativity_score']:.3f}\")\n",
        "                report.append(f\"**Token \u00c7e\u015fitlili\u011fi (BERT):** {result['metrics']['bert']['token_diversity']:.3f}\")\n",
        "            report.append(\"\")\n",
        "        report.append(\"## \ud83d\udca1 Sonu\u00e7 ve \u00d6neriler\")\n",
        "        if summary['successful_tests'] == summary['total_tests']:\n",
        "            report.append(\"\u2705 T\u00fcm testler ba\u015far\u0131l\u0131! Modeller stabil \u00e7al\u0131\u015f\u0131yor.\")\n",
        "        else:\n",
        "            report.append(f\"\u26a0\ufe0f {summary['failed_tests']} test ba\u015far\u0131s\u0131z. Hata loglar\u0131n\u0131 kontrol edin.\")\n",
        "        if summary['avg_bert_time'] < summary['avg_gpt2_time']:\n",
        "            report.append(\"\u26a1 BERT, metin analizi i\u00e7in daha h\u0131zl\u0131.\")\n",
        "        else:\n",
        "            report.append(\"\u26a1 GPT-2, metin \u00fcretimi i\u00e7in daha h\u0131zl\u0131.\")\n",
        "        if summary['avg_gpt2_words'] > 10:\n",
        "            report.append(\"\u2705 GPT-2 yeterli uzunlukta ve yarat\u0131c\u0131 metinler \u00fcretiyor.\")\n",
        "        else:\n",
        "            report.append(\"\u26a0\ufe0f GPT-2 metinleri k\u0131sa, temperature veya max_length art\u0131r\u0131labilir.\")\n",
        "        if summary['avg_bert_tokens'] > 5:\n",
        "            report.append(\"\u2705 BERT, metni etkili bir \u015fekilde analiz ediyor.\")\n",
        "        else:\n",
        "            report.append(\"\u26a0\ufe0f BERT token say\u0131s\u0131 d\u00fc\u015f\u00fck, daha uzun promptlar denenebilir.\")\n",
        "        report.append(\"\\n*Bu rapor otomatik olarak olu\u015fturulmu\u015ftur.*\")\n",
        "        return \"\\n\".join(report)\n",
        "    except Exception as e:\n",
        "        return f\"\u274c Rapor olu\u015fturma hatas\u0131: {e}\"\n",
        "\n",
        "# Test ve analiz y\u00fcr\u00fctme\n",
        "print(\"\ud83e\uddea Test ve analiz y\u00fcr\u00fct\u00fcl\u00fcyor...\")\n",
        "test_results = run_tests(test_prompts)\n",
        "\n",
        "# Test sonu\u00e7lar\u0131n\u0131 analiz et\n",
        "summary = analyze_test_results(test_results)\n",
        "\n",
        "# G\u00f6rselle\u015ftirme\n",
        "viz_fig = visualize_test_results(test_results, summary)\n",
        "if viz_fig:\n",
        "    print(\"\u2705 Test sonu\u00e7lar\u0131 g\u00f6rselle\u015ftirildi!\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\u274c G\u00f6rselle\u015ftirme hatas\u0131!\")\n",
        "\n",
        "# Nihai raporu olu\u015ftur\n",
        "final_report = generate_final_report(test_results, summary)\n",
        "print(\"\u2705 Nihai rapor olu\u015fturuldu!\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"\ud83d\udcc4 N\u0130HA\u0130 RAPOR (TAM):\")\n",
        "print(\"=\"*50)\n",
        "print(final_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llyFWTBk1VGR"
      },
      "source": [
        "## \ud83d\udccc **GPT-2 & BERT Kar\u015f\u0131la\u015ft\u0131rmal\u0131 Metin Testi \u00d6zeti**\n",
        "\n",
        "### \ud83e\udde0 Modellerin Y\u00fcklenmesi\n",
        "\n",
        "* \ud83d\udd39 **GPT-2** (\ud83d\udcdd Metin \u00fcretimi i\u00e7in)\n",
        "* \ud83d\udd39 **BERT** (\ud83d\udd0d Metin analizi i\u00e7in)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83e\uddea **Test Senaryolar\u0131**\n",
        "\n",
        "D\u00f6rt farkl\u0131 **prompt** kullan\u0131ld\u0131:\n",
        "\n",
        "1\ufe0f\u20e3 *\"The future of artificial intelligence is...\"*\n",
        "\n",
        "2\ufe0f\u20e3 *\"Climate change affects the world by...\"*\n",
        "\n",
        "3\ufe0f\u20e3 *\"In the digital age, privacy is...\"*\n",
        "\n",
        "4\ufe0f\u20e3 *\"Machine learning can revolutionize...\"*\n",
        "\n",
        "---\n",
        "\n",
        "### \u2699\ufe0f **\u0130\u015flem A\u015famalar\u0131**\n",
        "\n",
        "#### \u2728 GPT-2 (Metin \u00dcretimi)\n",
        "\n",
        "* Her prompt i\u00e7in **2 farkl\u0131 metin** \u00fcretildi\n",
        "* Ortalama **54.2 kelime**, yarat\u0131c\u0131 i\u00e7erik \ud83d\udca1\n",
        "* Yarat\u0131c\u0131l\u0131k Skoru: **0.85 - 0.93** \ud83c\udfa8\n",
        "\n",
        "#### \ud83e\udde9 BERT (Metin Analizi)\n",
        "\n",
        "* Token analizi yap\u0131ld\u0131 \u2705\n",
        "* **Attention a\u011f\u0131rl\u0131klar\u0131** g\u00f6rselle\u015ftirildi (heatmap) \ud83c\udf21\ufe0f\n",
        "* Ortalama **8 token**, \u00e7e\u015fitlilik: **1.0** \ud83c\udfaf\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udcca **Performans Kar\u015f\u0131la\u015ft\u0131rmas\u0131**\n",
        "\n",
        "| \u00d6zellik                | GPT-2     | BERT      |\n",
        "| ---------------------- | --------- | --------- |\n",
        "| \u23f1\ufe0f Ortalama S\u00fcre       | 0.468s    | 0.011s \u26a1  |\n",
        "| \ud83d\udcdd Kelime/Token Say\u0131s\u0131 | 54.2      | 8         |\n",
        "| \ud83c\udfa8 Yarat\u0131c\u0131l\u0131k         | 0.85-0.93 | -         |\n",
        "| \ud83d\udd01 Token \u00c7e\u015fitlili\u011fi   | 0.78      | **1.0** \u2b50 |\n",
        "\n",
        "\ud83d\udd0e **H\u0131z Oran\u0131 (BERT/GPT-2): 0.02** \u2014 BERT \u00e7ok daha h\u0131zl\u0131! \ud83c\udfce\ufe0f\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udcc8 **G\u00f6rselle\u015ftirme**\n",
        "\n",
        "1\ufe0f\u20e3 \u2705 **Ba\u015far\u0131 Oran\u0131**: Pie chart \ud83e\udd67\n",
        "2\ufe0f\u20e3 \u23f3 **Ortalama S\u00fcreler**: Bar chart \ud83d\udcca\n",
        "3\ufe0f\u20e3 \ud83d\udccf **Kelime/Token Say\u0131s\u0131**: Bar chart \ud83d\udcca\n",
        "4\ufe0f\u20e3 \ud83d\udd25 **BERT Attention A\u011f\u0131rl\u0131klar\u0131**: Heatmap \ud83c\udf21\ufe0f\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83e\uddfe **Nihai Rapor & \u00d6neriler**\n",
        "\n",
        "\ud83d\udccd Her iki model de **stabil** \u00e7al\u0131\u015ft\u0131\n",
        "\ud83d\udccd BERT = **h\u0131zl\u0131 ve verimli analiz**\n",
        "\ud83d\udccd GPT-2 = **yarat\u0131c\u0131 ve anlaml\u0131 metinler**\n",
        "\ud83d\udccd \ud83d\udccc \u00d6neriler:\n",
        "\n",
        "* Daha **uzun promptlar** test edilebilir\n",
        "* Parametrelerle oynanabilir: `temperature`, `max_length` \u2699\ufe0f\n",
        "\n",
        "---\n",
        "\n",
        "### \u2705 **Genel De\u011ferlendirme**\n",
        "\n",
        "\ud83d\udfe2 **BERT:** H\u0131zl\u0131 ve tutarl\u0131 analiz\n",
        "\ud83d\udfe2 **GPT-2:** G\u00fc\u00e7l\u00fc, yarat\u0131c\u0131 metin \u00fcretimi\n",
        "\ud83c\udfaf Her iki model kendi alan\u0131nda **ba\u015far\u0131l\u0131 ve kullan\u0131\u015fl\u0131**\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOe3NjyEae4cGUAaXgzdlFw",
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}